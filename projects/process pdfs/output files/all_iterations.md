2024-09-06 17:23:24.184561;

['# AWS Academy Cloud Architecting - Module 14: Planning for Disaster\n\n## Contents\n\n- Module 14: Planning for Disaster\n\n## Module Overview\n\n- **Sections:**\n    - Architectural need\n    - Disaster planning strategies\n    - Disaster recovery patterns\n- **Lab:**\n    - Guided Lab: Hybrid Storage and Data Migration with AWS Storage Gateway File Gateway\n- **Knowledge check:**\n\n## Module Objectives\n\nAt the end of this module, you should be able to:\n- Identify strategies for disaster planning\n- Define recovery point objective (RPO) and recovery time objective (RTO)\n- Describe four common patterns for backup and disaster recovery and how to implement them\n- Use AWS Storage Gateway for on-premises-to-cloud backup solutions\n\n## Section 1: Architectural need\n\nIntroducing Section 1: Architectural need.\n\n### Café business requirement\n\nIf the café\'s infrastructure ever becomes unavailable, the staff must be able to get their applications running again within an amount of time that is acceptable to the business. They need an architecture that supports their disaster recovery plans while also optimizing for cost.\n\nBy now, the café has implemented several applications that run on AWS. They are also storing a significant amount of business-critical data in the AWS Cloud. Sofía realizes that if the café\'s infrastructure ever becomes unavailable, they must be able to get their applications running and accessible in an amount of time that\'s acceptable to the business. Currently, the café\'s staff hasn\'t developed any comprehensive disaster recovery plans.\n\nSofía raised this concern with Frank and Martha. They all agreed that it\'s important to put backup and disaster recovery plans into place. Their objective is to implement an architecture that supports their disaster recovery time objectives, while it also optimizes for cost. They also agreed that as their revenue grows, they will be able to afford a solution that supports a shorter recovery time objective.\n\nIn this module, you will learn about key AWS service features that support data backup and disaster recovery. With an understanding of these features, you should be able to help the café meet this essential business requirement.\n\n## Section 2: Disaster planning strategies\n\nIntroducing Section 2: Disaster planning strategies.\n\n### Planning for failures\n\n"Everything fails, all the time." -Werner Vogels\n\nThe Chief Technology Officer (CTO) of AWS, Werner Vogels, has famously stated on more than one occasion that, "Everything fails, all the time." His pronouncement has continued to influence cloud computing architectural design for many years, because it speaks to a truism.\n\nFailure should not be thought of as an unlikely aberration. Instead, it should be assumed that failures, both large and small, can-and will-occur. How do you prepare for these events?\n\nA failure can be categorized as one of three types:\n\n- A small-scale event – For example, a single server stopped responding or went offline\n- A large-scale event – In this case, multiple resources were affected, perhaps even across Availability Zones within a Region\n- A colossal scale event – In this case, the failure is widespread, and it affects a large number of users and systems\n\nTo minimize the impact of a disaster, organizations must invest time and resources to plan and prepare, to train employees, and to document and update processes. The amount of investment for disaster planning for a particular system can vary dramatically, depending on the cost of a potential outage.\n\n### Avoiding and planning for disaster\n\nYou can work to avoid and plan for disaster in three ways:\n\n- **High availability** provides redundancy and fault tolerance. A system is highly available when it can withstand failure of an individual or multiple components (for example, hard disks, servers, or network connectivity). Production systems typically have defined uptime requirements.\n- **Backup** is critical to protecting data and ensuring business continuity. However, it can be a challenge to implement. The pace at which data is generated is growing exponentially. Meanwhile, the density and durability of local disks are not experiencing the same growth rate. Even so, it is essential to keep your critical data backed up, in case of disaster.\n- **Disaster recovery (DR)** is about preparing for and recovering from a disaster. A disaster is any event that has a negative impact on a company\'s business continuity or finances. Such events include hardware or software failure, a network outage, a power outage, or physical damage to a building (like fire or flooding). The cause can be human error, or some other significant event. Disaster recovery is a set of policies and procedures that enable the recovery or continuation of vital technology infrastructure and systems after any disaster.\n\n### Selected AWS Well-Architected Framework design principles\n\n#### Operational Excellence pillar\n\n- Anticipate failure\n- Refine operational procedures frequently\n\n#### Reliability pillar\n\n- Test recovery procedures\n- Automatically recover from failure\n\nConsider some design principles that relate to the topic of disaster recovery.\n\nThe Operational Excellence pillar of the AWS Well-Architected Framework states the importance of anticipating failure. It recommends that you perform pre-mortem exercises to identify potential sources of failure so that they can be removed or mitigated. You must test your failure scenarios and validate your understanding of their impact. The AWS Well-Architected Framework also describes the benefits of refining your operational procedures frequently so that you can look for opportunities to improve them. Then, as you evolve your workload, you can evolve your procedures accordingly.\n\nThe Reliability pillar describes the importance of designing your systems. You must be able to recover from infrastructure or service disruptions, and mitigate disruptions such as misconfigurations or transient network issues.\n\nOne of the design principles that it mentions is to test recovery procedures. Test how your system fails, and validate your recovery procedures. You can use automation to simulate different failures or to recreate scenarios that led to previous failures. This testing exposes failure pathways that you can test and fix before a real failure scenario. It reduces the risk of components that have not been tested before they fail.\n\nAnother principle of design is to automatically recover from failure. By monitoring a system for key performance indicators (KPIs), you can trigger automation when a threshold is breached. These KPIs should be a measure of business value, not the technical aspects of how the service operates. Your automation could provide notifications and tracking of failures, and for automated recovery processes that work around or repair the failure.\n\n### Recovery point objective (RPO)\n\nRecovery point objective (RPO) is the maximum acceptable amount of data loss, measured in time.\n\nHow often must your data be backed up?\n\nExample RPO: The business can recover from losing (at most) the last 8 hours of data.\n\nOrganizations of all sizes, large and small, often have a Business Continuity Plan (BCP). A typical part of the BCP is to provide for IT Service Continuity, including IT disaster recovery planning.\n\nOne of the most important measures of a disaster recovery plan is to define your recovery point objective (RPO). To calculate RPO, first determine how much data loss is acceptable, according to your BCP. Then, figure out how quickly that data loss might occur, as a time measurement.\n\nFor example, suppose you determine that the data that your application generates is important but not critical, so that losing 800 records would be acceptable. You further calculate that even during peak times, no more than 100 records are created in an hour. In this scenario, you decide that an RPO of 8 hours is sufficient to meet your needs. If you then implement a disaster recovery plan that meets this RPO, you are sure to do data backups at least every 8 hours. Then, if a disaster occurs at 22:00, the system should be able to recover all data that was in the system before 14:00 PM.\n\n### Recovery time objective (RTO)\n\nRecovery time objective (RTO) is the maximum acceptable amount of time after disaster strikes that a business process can remain out of commission.\n\nHow quickly must your applications and data be recovered?\n\nExample RTO: The application can be unavailable for a maximum of 1 hour.\n\nAnother important measure of a disaster recovery plan is to define the recovery time objective (RTO). RTO is the time that it takes after a disruption to restore your applications and recover your data. To continue the previous example, suppose a disaster occurs at 22:00 and the RTO is 1 hour. In that scenario, the DR process should restore the business process to the acceptable service level by 23:00.\n\nA company typically decides on acceptable RPO and RTO, and it bases its decision on the financial impact to the business when systems are unavailable. The company determines financial impact by considering many factors. These factors include loss of business and damage to its reputation because of downtime and the lack of systems availability.\n\nIT organizations then plan solutions to provide cost-effective system recovery. The solutions are based on the RPO within the timeline and the service level that the RTO establishes.\n\n### Plan for disaster recovery\n\nBe intentional about where your data is stored and where your applications run.\n\nTo properly scope your disaster recovery planning, you must look holistically at your use of AWS. Most organizations use a combination of services that can be broadly categorized as encompassing these five service categorizes areas:\n\n- Storage\n- Compute\n- Networking\n- Databases\n- Deployment orchestration services\n\nIf a disaster occurs, your RPO and RTO will guide your backup-and-restore plans and procedures across each of these service areas. They will also likely affect your production deployment architecture.\n\nIt is also important to keep in mind that, although it\'s unlikely for a Region to be unavailable, it is within the realm of possibility. If some large-scale event affects a Region—for instance, a meteor strike-would your data still be available? Would your applications still be accessible? AWS provides multiple Regions around the world. Thus, you can choose the most appropriate location for your disaster recovery site, in addition to the site where your system is fully deployed.\n\n### Storage and backup building blocks\n\nThe following services are referenced in the diagram:\n\n- Amazon Elastic Block Store (Amazon EBS)\n- Amazon Elastic Compute Cloud (Amazon EC2)\n- Amazon Elastic File System (Amazon EFS)\n- Amazon Simple Storage Service (Amazon S3)\n- Amazon Simple Storage Service Glacier (Amazon S3 Glacier)\n\nTo start your disaster planning in detail, look at the data storage layer (postponing the discussion of database layer for the moment).\n\nYour AWS Cloud storage can consists of a combination of block storage, file system storage, and object storage. Meanwhile, your organization might also use AWS services that connect the on-premises data center to the AWS Cloud.\n\nIn the next few slides, you will learn about high-level best practices for each of these three areas.\n\nOne service that you might not be familiar with is AWS DataSync. AWS DataSync provides movement of large amounts of data online between on-premises storage and Amazon S3, Amazon EFS, or Amazon FSx for Windows File Server. It supports scripted copy jobs and scheduled data transfers from on-premises Network File Systems (NFS) and Server Message Block (SMB) storage. It can also optionally use AWS Direct Connect links.\n\n### Best practice: S3 Cross-Region Replication\n\nMost S3 storage classes replicate data across Availability Zones within a single Region\n\n- Configure S3 cross-Region replication for higher-level data security\n- Automatically, asynchronously replicates objects created after you add the replication configuration\n- Can also help meet compliance requirements and reduce latency for users who are accessing objects\n\nFor many organizations, the bulk of their data that is stored on AWS is in Amazon S3, which provides object storage.\n\nRecall that S3 buckets exist in a specific AWS Region. You choose the Region when you create the bucket. Amazon S3 provides 11 9s (99.999999999 percent) of durability for S3 Standard, S3 Standard-IA, S3 One Zone-IA, and Amazon S3 Glacier storage classes. Amazon S3 Standard, S3 Standard-IA, and Amazon S3 Glacier are all designed to sustain data if an entire Amazon S3 Availability Zone loss occurs. They provide this stability by automatically storing your objects across a minimum of three Availability Zones, each separated miles apart, across a single AWS Region.\n\nFor critical applications and data scenarios where you want a higher level of data security, it is a best practice to configure S3 cross-Region replication. To enable the replication, you add a replication configuration to your source bucket. The minimum configuration must indicate the destination bucket where you want Amazon S3 to replicate all objects, or a subset of all objects. It must also include an AWS Identity and Access Management (IAM) role that grants Amazon S3 permissions to copy the objects to the destination bucket.\n\nCopied objects retain their metadata. The destination bucket can belong to another storage class. For example, the contents of an S3 Standard bucket might be replicated to an Amazon S3 Glacier bucket. You can assign different ownership to the objects in the destination bucket. You can also use S3 Replication Time Control (S3 RTC) to replicate your data across different Regions in a predictable time frame. S3 RTC replicates four 9s (99.99 percent) of new objects stored in Amazon S3 within 15 minutes (backed by a service-level agreement).\n\n### Best practice: EBS volume snapshots\n\nCreate point-in-time snapshots of EBS volumes\n\n- Snapshots provide incremental backups (they back up the blocks that changed since the previous snapshot)\n- Snapshots enable you to restore data to a new EBS volume\n- Use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of snapshots\n- You cannot snapshot instance storage\n\nRegarding to block storage, you can back up the data that is on EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that it saves only the blocks on the device that have changed since your most recent snapshot. This architecture minimizes the time that is required to create the snapshot, and it saves on storage costs by not duplicating data.\n\nEach snapshot contains all the information that is needed to restore your data (from the moment when the snapshot was taken) to a new EBS volume. When you create an EBS volume that is based on a snapshot, the new volume begins as an exact replica of the original volume. This original volume was used to create the snapshot. The replicated volume loads data in the background so that you can begin to use it immediately. If you access data that has not been loaded yet, the volume immediately downloads the requested data from Amazon S3. Then, it continues to load the rest of the volume\'s data in the background.\n\nAmazon EBS volumes provide off-instance storage that persists independently from the life of an instance, and is replicated across multiple servers in an Availability Zone. Volumes prevent the loss of data from the failure of any single component. After you create a snapshot, it finishes copying to Amazon S3 (when the snapshot status is completed). Then, you can copy it from one AWS Region to another, or within the same Region.\n\nYou can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of snapshots that back up your EBS volumes. Automating snapshot management helps you to:\n\n- Protect valuable data by enforcing a regular backup schedule\n- Retain backups as required by auditors or internal compliance\n- Reduce storage costs by deleting outdated backups\n\nYou cannot create snapshots of EC2 instance store volumes. However, if you must back up data from an instance store, you can create a new EBS volume and format it. Then, mount the new volume to the EC2 instance guest OS and copy the data on your instance store volume to the EBS volume.\n\nRecall that instance store volumes provide temporary block-level storage that works well for information that changes frequently, such as buffers, caches, and scratch data. You might find that you must back up data from an instance store. If so, you might want to rethink why you are storing that data on an instance store volume in the first place.\n\n### Best practice: File system replication\n\nIt is also a best practice to replicate your file storage.\n\nAWS DataSync makes data move faster between two EFS or Amazon FSx Windows File Server file systems, or between on-premises storage and AWS file storage. You can use DataSync to transfer datasets over DX or the internet. Use the service for one-time data migrations or ongoing workflows for data protection and recovery.\n\nYou can learn more about how to use AWS Backup to manage EBS volume backups and to automate backups of EFS file systems. See the Scheduling automated backups using Amazon EFS and AWS Backup blog for details.\n\nFSx for Windows File Server takes daily automatic backups of your file systems, and it enables you to take more backups at any point. Amazon FSx stores the backups in Amazon S3. The daily backup window is a 30-minute window that you specify when you create a file system. The daily backup retention period that is specified for your file system determines the number of days that your daily automatic backups are kept. (This number is 7 days by default.)\n\nLike most Amazon S3 storage classes replicate data across Availability Zones, so do Amazon EFS and FSx for Windows File Server file systems. Your disaster recovery requirements might specify that you need a multi-region recovery solution. In that case, it is a best practice to replicate your Amazon EFS and FSx for Windows File Server file systems to a second Region. You can use AWS DataSync to get this replication. To simplify file transfer between two EFS file systems by using DataSync, you can use the AWS DataSync In-Cloud QuickStart and Scheduler.\n\n### Compute capacity should be quickly recoverable\n\nObtain and boot new server instances or containers within minutes.\n\nIn the context of DR, it\'s critical that you can rapidly create virtual machines that you control. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location.\n\nYou can arrange for automatic recovery of an EC2 instance when a system status check of the underlying hardware fails. The instance is rebooted (on new hardware, if necessary)—but it retains its instance ID, IP addresses, EBS volume attachments, and other configuration details. For a complete recovery, make sure that the instance is configured to automatically start up any services or applications as part of its initialization process.\n\nAmazon Machine Images (AMIs) are preconfigured with operating systems, and some preconfigured AMIs might also include application stacks. You can also configure your own custom AMIs. In the context of DR, AWS recommends that you configure and identify your own AMIs so that they launch as part of your recovery procedure. Such AMIs should be preconfigured with your operating system of choice, in addition to the appropriate pieces of the application stack.\n\n### Strategies for compute disaster recovery\n\n- Use the Amazon EC2 snapshot capability for backups\n- Snapshots can be performed manually, or scheduled (for example, by using AWS Lambda)\n- Use system or instance level system backups infrequently and as a last resort\n- Drives up the cost of storage that is used quickly\n- Prefer automated rebuild from configuration or code repositories instead\n- Cross-region AMI copies\n- Cross-region snapshot copies\n- Consider transient compute architectures\n- Store essential data off the instance\n\nFor disaster recovery of compute resources, you will probably want to use the Amazon EC2 snapshot capability. Snapshots can be performed manually, or they can be scheduled.\n\nAlthough you can create system or instance-level system backups, extensive use of this approach increases your storage costs. A better approach is to configure an automated rebuild process, where your source code is stored in a repository.\n\nYou might want to replicate Amazon S3 across Regions, and you probably also want to replicate your most critical AMIs and snapshots across Regions.\n\nFinally, consider architecting your use of compute resources to store essential data off of the instances. As you see in the example, your data can be stored in an S3 bucket. When you must do data processing, you can launch one or more EC2 instances from a custom AMI that is preconfigured with application software. As soon as the instance is started, it can pull the needed data from the S3 bucket and process the data. Then, it can write the output data back to Amazon S3 (perhaps to another S3 bucket). After the instance completes its compute tasks, the instance can be terminated. Such an architecture—when it can still meet your business needs-makes it easier to design your disaster recovery strategy. It also can save on costs, because servers that are not in constant use can be terminated and then later re-created when needed.\n\n### Networking: Design for resilience, recovery\n\n- **Amazon Route 53**\n    - Traffic distribution\n    - Failover\n- **Elastic Load Balancing**\n    - Load balancing\n    - Health checks and failover\n- **Amazon Virtual Private Cloud (Amazon VPC)**\n    - Extend your existing on-premises network topology to the cloud\n- **AWS Direct Connect**\n    - Fast, consistent replication and backups of large on-premises environments to the cloud\n\nWhen you work to recover from a disaster, it\'s likely that you must modify network settings to fail your system over to another site. AWS offers several services and features that enable you to manage and modify network settings, a few of which are highlighted next.\n\nAmazon Route 53 provides load balancing and network routing capabilities that enable you to distribute network traffic. It also provides the ability to fail over between multiple endpoints and even to a static website that is hosted in Amazon S3.\n\nThe Elastic Load Balancing service automatically distributes incoming application traffic across multiple EC2 instances. It enables you to achieve fault tolerance in your applications by providing the load-balancing capacity that is needed in response to incoming application traffic. You can pre-allocate a load balancer so that its Domain Name System (DNS) name is already known, which can simplify implementation of your DR plan.\n\nYou can use Amazon Virtual Private Cloud (Amazon VPC) to extend an existing on-premises network topology to the cloud. This extension can be especially appropriate when you recover enterprise applications that might be hosted on an internal network.\n\nFinally, AWS Direct Connect simplifies the setup of a dedicated network connection from an on-premises data center to AWS. Using DX can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections.\n\n### Databases: Features that support recovery\n\n- **Amazon Relational Database Service (Amazon RDS)**\n    - Take snapshot data and save it in a separate Region\n    - Combine read replicas with Multi-AZ deployments to build a resilient disaster recovery strategy\n    - Retain automated backups\n- **Amazon DynamoDB**\n    - Back up entire tables in seconds\n    - Use point-in-time-recovery to continuously back up tables for up to 35 days\n    - Initiate backups with a single click in the console or a single application programming interface (API) call\n    - Use Global Tables to build a multi-region, multi-master database that provides fast local performance for massively scaled globally distributed applications\n\nAWS provides many database services. Some key features of Amazon RDS and Amazon DynamoDB that are relevant to disaster recovery scenarios are explained next.\n\nConsider using Amazon RDS in the DR preparation phase to store a copy of your critical data in a database that is already running. Then, use Amazon RDS in the DR recovery phase to run your production database.\n\nIf you implement a multi-region DR plan, Amazon RDS gives you the ability to store snapshot data that was captured from one Region to another Region. You can share a manual snapshot with up to 20 other AWS accounts.\n\nCombining read replicas with Multi-AZ deployments enables you to build a resilient disaster recovery strategy and simplify your database engine upgrade process. By using Amazon RDS read replicas, you can create one or more read-only copies of your database instance. You can create these copies within the same AWS Region, or in a different AWS Region. Updates to the source database are then asynchronously copied to your read replicas. Read replicas can be promoted to become a standalone database instance, when needed.\n\nUse Amazon DynamoDB in the preparation phase to copy data to DynamoDB in another Region or to Amazon S3. During the recovery phase of DR, you can scale up in minutes. DynamoDB global tables replicate your DynamoDB tables automatically across your choice of AWS Regions. They resolve update conflicts and enable your applications to stay highly available, even in the unlikely event that an entire Region is isolated or affected by degradation.\n\n### Automation services: Quickly replicate or redeploy environments\n\n- **AWS CloudFormation**\n    - Use templates to quickly deploy collections of resources as needed\n    - Duplicate production environments in new Region or VPC in minutes\n- **AWS Elastic Beanstalk**\n    - Quickly redeploy your entire stack in only a few clicks\n- **AWS OpsWorks**\n    - Automatic host replacement\n    - Combine it with AWS CloudFormation in the recovery phase\n    - Provision a new stack that supports the defined RTO\n\nWhen you use automation services, you can quickly replicate or redeploy environments.\n\nAWS CloudFormation enables you to model and deploy your entire infrastructure in a text file. This template can become the single source of truth for your infrastructure. When you use AWS CloudFormation to manage your entire infrastructure, it also becomes a powerful tool in your disaster recovery planning toolkit. It enables you to duplicate complex production environments in minutes, for example, to a new Region or a new VPC.\n\nAWS CloudFormation provisions your resources in a repeatable manner, which enables you to build and rebuild your infrastructure and applications. You are not required to perform manual actions or write custom scripts.\n\nIf you use AWS Elastic Beanstalk to host your applications, you can upload an updated application source bundle and deploy it to your AWS Elastic Beanstalk environment. Alternatively, you can redeploy a previously uploaded version of an application. You can also deploy a previously uploaded version of your application to any of its environments.\n\nFinally, AWS OpsWorks is an application management service that makes it easy to deploy and operate applications of all types and sizes. You can define your environment as a series of layers, and configure each layer as a tier of your application. AWS OpsWorks has automatic host replacement, so if you have an instance failure, it is automatically replaced. You can use AWS OpsWorks in the DR preparation phase to template your environment and combine it with AWS CloudFormation in the DR recovery phase.\n\n## Section 2 key takeaways\n\n- To choose the correct disaster recovery strategy, first identify your recovery point objective (RPO) and recovery time objective (RTO)\n- Use features such as S3 Cross-Region Replication, EBS volume snapshots, and Amazon RDS snapshots to protect data\n- Use networking features (such as Route 53 failover and Elastic Load Balancing) to improve application availability\n- Use automation services (such as AWS CloudFormation) as part of your DR strategy to quickly deploy duplicate environments when needed\n\nSome key takeaways from this section of the module include:\n\n- To choose the correct disaster recovery strategy, first identify your recovery point objective (RPO) and recovery time objective (RTO)\n- Use features such as S3 Cross-Region Replication, EBS volume snapshots, and RDS snapshots to protect data\n- Use networking features-such as Route 53 failover and Elastic Load Balancing—to improve application availability\n- Use automation services—such as AWS CloudFormation—as part of your DR strategy to quickly deploy duplicate environments when necessary\n\n## Section 3: Disaster recovery patterns\n\nIntroducing Section 3: Disaster recovery patterns.\n\n### Common disaster recovery patterns on AWS\n\nFour disaster recovery patterns\n\n- Backup and restore\n- Pilot light\n- Warm standby\n- Multi-site\n\nEach pattern is suited to a different combination of:\n\n- Recovery point objective\n- Recovery time objective\n- Cost-effectiveness\n\nOrganizations often use these four common disaster recovery patterns:\n\n- Backup and restore\n- Pilot light\n- Warm standby\n- Multi-site\n\nAs you will discover in the details that follow, each pattern is well-suited to different requirements. Some of the patterns offer better cost-effectiveness. Others provide a faster RPO and faster RTO, but cost more to maintain.\n\n### Backup and restore pattern\n\nBack up configuration and state data to S3. Implement lifecycle policy to save on cost.\n\nThe first disaster recovery approach is the backup and restore pattern.\n\nIn most traditional environments, data is backed up to tape and sent offsite regularly. If you use this method, it can take a long time to restore your system when a disaster occurs.\n\nAmazon S3 provides a more easily accessible destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location.\n\nIn the example backup scenario, data is copied from the on-premises data center to Amazon S3. AWS DataSync or Amazon S3 Transfer Acceleration can optionally be used as part of this configuration to automate or increase the speed of data transfer. Then, an S3 lifecycle configuration that is applied to the bucket later moves the backup data to less-expensive Amazon S3 storage classes. The backup data moves to Amazon S3 Glacier or Amazon S3 Standard-IA, which saves on cost as the data ages and is not frequently accessed.\n\nIn the example restore scenario, the on-premises data might be temporarily or permanently lost. Then, the backup data can be downloaded from Amazon S3 back to the on-premises servers.\n\nIf your corporate data center remains offline, you can further ensure the ability to restore your data to your servers. You can have Amazon EC2 servers that are ready to go in a VPC in your designated disaster recovery Region. This Region can connect to the S3 bucket that contains your backup application data. It can read that data, and perhaps temporarily host your applications while you work to restore your data center.\n\n### AWS Storage Gateway\n\nAs part of the backup and restore pattern, you might find that it makes sense to use AWS Storage Gateway.\n\nAWS Storage Gateway is a hybrid storage service that enables your on-premises applications to use AWS Cloud storage. You can use the service for backup and archiving, disaster recovery, cloud data processing, storage tiering, and migration.\n\nYour applications connect to the service through a virtual machine or hardware gateway appliance by using standard storage protocols. These protocols include NFS, SMB, Virtual Tape Library (VTL), and Internet Small Computer System Interface (iSCSI). The gateway connects to AWS storage services-such as Amazon S3, Amazon S3 Glacier, and Amazon EBS-which provide storage for files, volumes, and virtual tapes. The service includes an optimized data transfer mechanism. It provides bandwidth management, automated network resilience, and efficient data transfer, in addition to a local cache for low-latency on-premises access to your most active data.\n\nWith a file gateway, you store and retrieve objects (by using the NFS or SMB protocol) in Amazon S3. You use a local cache for low-latency access to your most recently used data. When your files are transferred to Amazon S3, they are stored as objects and can be accessed through an NFS mount point.\n\nThe Storage Gateway volume interface presents your applications with block storage disk volumes that can be accessed by using the iSCSI protocol. Data on these volumes is backed up as point-in-time EBS snapshots, which enables you to access it through Amazon EC2, if needed.\n\nThe Storage Gateway tape interface presents the Storage Gateway to your existing backup application as a virtual tape library. This library consists of a virtual media changer and virtual tape drives. You can continue to use your existing backup applications while you write to a collection of virtual tapes. Each virtual tape is stored in Amazon S3. When you no longer require access to data on virtual tapes, your backup application archives it from the virtual tape library into Amazon S3 Glacier.\n\n### Backup and restore: Checklist\n\n#### Preparation phase\n\n- Create backups of current systems\n- Store backups in Amazon S3\n- Document procedure to restore from backups\n- Know:\n    - Which AMI to use, and build as needed\n    - How to restore system from backups\n    - How to route traffic to the new system\n    - How to configure the deployment\n\n#### In case of disaster\n\n- Retrieve backups from Amazon S3\n- Restore required infrastructure\n    - EC2 instances from prepared AMIS\n    - Elastic Load Balancing load balancers\n    - AWS resources created by an AWS CloudFormation stack - automated deployment to restore or duplicate the environment\n- Restore system from backup\n- Route traffic to the new system\n- Adjust Domain Name System (DNS) records accordingly\n\nIf you implement the backup and restore disaster recovery pattern, the key steps that you should complete during the preparation phase are:\n\n- Create backups of current systems\n- Store backups in Amazon S3\n- Document the procedure to restore from backups\n\nIf you implement this pattern, the key steps to complete in case of disaster are:\n\n- Retrieve backups from Amazon S3\n- Start the required infrastructure\n- Restore the system from backups\n- Finally, route traffic to the new system\n\n### Pilot light pattern: Preparation phase\n\nThe second disaster recovery approach is the pilot light pattern.\n\nPilot light describes a disaster recovery pattern where a minimal backup version of your environment is always running. The pilot light analogy comes from a gas heater: a small flame (or the pilot light) is always on, even when the heater is off. The pilot light can quickly ignite the entire furnace to heat a house. In the example pattern, the pilot light is the secondary database that is always running.\n\nThe pilot light scenario is similar to the backup-and-restore scenario. However, recovery time is typically faster because the core pieces of the system are already running and are continually kept up-to-date. When the time comes for recovery, you can rapidly provision a full production environment around the critical core.\n\nInfrastructure elements for the pilot light itself typically include your database servers. This grouping is the critical core of the system (the pilot light). All other infrastructure pieces can quickly be provisioned around it to restore the complete system. To provision the rest of the infrastructure, you typically bundle preconfigured servers as AMIs that are ready to be started at a moment\'s notice. (Or they might be instances that are in a stopped state.) When recovery begins, these instances start quickly with their pre-defined role, which enables them to connect to the database.\n\nThis pattern is relatively inexpensive to implement. Regularly changing data must be replicated to the pilot light, the small core around which the full environment starts in the recovery phase. Your less frequently updated data, such as operating systems and applications, can be periodically updated and stored as AMIs.\n\n### Pilot light pattern: In case of disaster\n\nSuppose that disaster strikes, and your primary application goes offline. In this case, you can quickly commission the compute resources to run the application or to orchestrate the failover to pilot light resources in AWS. In this example, the secondary database stores critical data. If there is a disaster, the new web server and app server start up and connect to the secondary database. Amazon Route 53 is configured to then route traffic to the new web server.\n\nThe primary environment can exist in an on-premises data center, or in another Region or Availability Zone on AWS. Either way, you can use the pilot light pattern to meet your recovery time objective (RTO).\n\n### Pilot light pattern: Checklist\n\n#### Preparation phase\n\n- Configure EC2 instances to replicate or mirror servers\n- Ensure that all supporting custom software packages are available\n- Create and maintain AMIs of key servers where fast recovery is needed\n- Regularly run these servers, test them, and apply any software updates and configuration changes\n- Consider automating the provisioning of AWS resources\n\n#### In case of disaster\n\n- Automatically bring up resources around the replicated core dataset\n- Scale the system as needed to handle current production traffic\n- Switch over to the new system\n- Adjust DNS records to point to AWS\n\nIf you implement the pilot light disaster recovery pattern, the key steps that you should complete during the preparation phase are:\n\n- Configure the EC2 instances\n- Ensure that all of the supporting custom software packages are available\n- Create and maintain essential AMIs where fast recovery is required\n- Regularly run and test servers, and apply software updates and configuration updates\n- Consider automating the provisioning of AWS resources\n\nIf you implement the pilot light pattern, the key steps to complete in case of disaster are:\n\n- Automatically bring up resources around the replicated core dataset\n- Scale the system as needed to handle current production traffic\n- Switch over to the new system by adjusting the DNS records to point to the backup deployment\n\n### Warm standby pattern: Preparation phase\n\nThe third disaster recovery approach is the warm standby pattern.\n\nThe warm standby pattern is like the pilot light, but more resources are already running. The term warm standby describes a disaster recovery scenario where a scaled-down version of a fully functional environment is always running in the cloud. The warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems and have them always on.\n\nThese servers can be running on a minimum-sized fleet of EC2 instances with the smallest sizes possible. This solution is not yet scaled to take a full production load, but it is fully functional. Though it exists for DR purposes, you can also use it for non-production work, such as testing, quality assurance, and internal use.\n\nIn the example, two systems are running. The main system might be running in an on-premises data center or an AWS Region, and a low-capacity system is running on AWS. Use Amazon Route 53 to distribute requests between the main system and the backup system.\n\n### Warm standby pattern: In case of disaster\n\nIn a disaster, if the primary environment is unavailable, Amazon Route 53 switches over to the secondary system.\n\nThe secondary system can then quickly begin to scale up to handle the production load. You can produce this increase by adding more EC2 instances to the load balancer. Alternatively, you can resize the small capacity servers to run on larger EC2 instance types. Horizontal scaling (creating more EC2 instances) is preferred over vertical scaling (increasing the size of existing instances).\n\n### Warm standby pattern: Checklist\n\n#### Preparation\n\n- Similar to pilot light\n- All necessary components running 24/7, but not scaled for production traffic\n- Best practice: Continuous testing\n- Trickle a statistical subset of production traffic to the DR site\n\n#### In case of disaster\n\n- Immediately fail over most critical production load\n- Adjust DNS records to point to AWS\n- (Automatically) Scale the system further to handle all production load\n\nIf you implement the warm standby disaster recovery pattern, the preparation phase is important. The key steps that you should complete during the preparation phase are similar to the steps that you complete for the pilot light pattern. The most notable difference is that all the necessary components should be left running 24/7, but not scaled for production traffic.\n\nAs a best practice, conduct continuous testing. You might also trickle a statistical subset of', '# AWS Academy Cloud Architecting - Module 14: Planning for Disaster\n\n## 1. Contents\n\n- Module 14: Planning for Disaster\n\n## 2. Module Overview\n\n- **Sections:**\n    - 2.1. Architectural need\n    - 2.2. Disaster planning strategies\n    - 2.3. Disaster recovery patterns\n- **Lab:**\n    - Guided Lab: Hybrid Storage and Data Migration with AWS Storage Gateway File Gateway\n- **Knowledge check:**\n\n## 3. Module Objectives\n\nAt the end of this module, you should be able to:\n- Identify strategies for disaster planning.\n- Define recovery point objective (RPO) and recovery time objective (RTO).\n- Describe four common patterns for backup and disaster recovery and how to implement them.\n- Use AWS Storage Gateway for on-premises-to-cloud backup solutions.\n\n## 4. Section 1: Architectural Need\n\n### 4.1. Café Business Requirement\n\nIf the café\'s infrastructure ever becomes unavailable, the staff must be able to get their applications running again within an amount of time that is acceptable to the business. They need an architecture that supports their disaster recovery plans while also optimizing for cost.\n\nThe café has implemented several applications that run on AWS and is storing a significant amount of business-critical data in the AWS Cloud.  Sofía, a member of the café\'s team, realizes that if the café\'s infrastructure ever becomes unavailable, they must be able to get their applications running and accessible within an acceptable time frame. Currently, the café\'s staff hasn\'t developed any comprehensive disaster recovery plans.\n\nSofía raised this concern with Frank and Martha. They all agreed that it\'s important to put backup and disaster recovery plans into place. Their objective is to implement an architecture that supports their disaster recovery time objectives while optimizing for cost. They also agreed that as their revenue grows, they will be able to afford a solution that supports a shorter recovery time objective.\n\nIn this module, you will learn about key AWS service features that support data backup and disaster recovery. With an understanding of these features, you should be able to help the café meet this essential business requirement.\n\n## 5. Section 2: Disaster Planning Strategies\n\n### 5.1. Planning for Failures\n\n"Everything fails, all the time." -Werner Vogels\n\nAWS Chief Technology Officer (CTO), Werner Vogels, has famously stated on more than one occasion that, "Everything fails, all the time." This statement has been influential in cloud computing architectural design because it highlights a critical truth: failure is inevitable.\n\nWe should not consider failure as an unlikely aberration. Instead, we should assume that failures, both large and small, can and will occur. How do we prepare for these events?\n\nFailures can be categorized into three types:\n\n- **Small-scale event:** A single server stopped responding or went offline.\n- **Large-scale event:** Multiple resources were affected, perhaps even across Availability Zones within a Region.\n- **Colossal scale event:** The failure is widespread, affecting a large number of users and systems.\n\nTo minimize the impact of a disaster, organizations must invest time and resources to plan and prepare, train employees, and document and update processes. The amount of investment for disaster planning for a particular system can vary dramatically, depending on the cost of a potential outage.\n\n### 5.2. Avoiding and Planning for Disaster\n\nWe can work to avoid and plan for disaster in three ways:\n\n- **High availability:** Provides redundancy and fault tolerance. A system is highly available when it can withstand the failure of one or multiple components (e.g., hard disks, servers, or network connectivity). Production systems typically have defined uptime requirements.\n- **Backup:** Critical to protecting data and ensuring business continuity. However, it can be challenging to implement. The pace at which data is generated is growing exponentially, while the density and durability of local disks are not experiencing the same growth rate. Even so, it is essential to keep your critical data backed up in case of disaster.\n- **Disaster recovery (DR):** About preparing for and recovering from a disaster. A disaster is any event that has a negative impact on a company\'s business continuity or finances. Such events include hardware or software failure, a network outage, a power outage, or physical damage to a building (like fire or flooding). The cause can be human error, or some other significant event. Disaster recovery is a set of policies and procedures that enable the recovery or continuation of vital technology infrastructure and systems after any disaster.\n\n### 5.3. Selected AWS Well-Architected Framework Design Principles\n\n#### 5.3.1. Operational Excellence Pillar\n\n- Anticipate failure.\n- Refine operational procedures frequently.\n\n#### 5.3.2. Reliability Pillar\n\n- Test recovery procedures.\n- Automatically recover from failure.\n\nConsider some design principles that relate to the topic of disaster recovery.\n\nThe **Operational Excellence pillar** of the AWS Well-Architected Framework emphasizes the importance of anticipating failure. It recommends performing pre-mortem exercises to identify potential sources of failure so that they can be removed or mitigated. You must test your failure scenarios and validate your understanding of their impact. The AWS Well-Architected Framework also describes the benefits of refining your operational procedures frequently to look for opportunities to improve them. Then, as you evolve your workload, you can evolve your procedures accordingly.\n\nThe **Reliability pillar** describes the importance of designing systems that can recover from infrastructure or service disruptions, and mitigating disruptions such as misconfigurations or transient network issues.\n\nOne of the design principles it mentions is to **test recovery procedures**. Test how your system fails and validate your recovery procedures. You can use automation to simulate different failures or recreate scenarios that led to previous failures. This testing exposes failure pathways that you can test and fix before a real failure scenario. It reduces the risk of components that have not been tested before they fail.\n\nAnother principle of design is to **automatically recover from failure**. By monitoring a system for key performance indicators (KPIs), you can trigger automation when a threshold is breached. These KPIs should be a measure of business value, not the technical aspects of how the service operates. Your automation could provide notifications and tracking of failures, and for automated recovery processes that work around or repair the failure.\n\n### 5.4. Recovery Point Objective (RPO)\n\n**Recovery point objective (RPO)** is the maximum acceptable amount of data loss, measured in time.\n\n**How often must your data be backed up?**\n\n**Example RPO:** The business can recover from losing (at most) the last 8 hours of data.\n\nOrganizations of all sizes, large and small, often have a Business Continuity Plan (BCP). A typical part of the BCP is to provide for IT Service Continuity, including IT disaster recovery planning.\n\nOne of the most important measures of a disaster recovery plan is to define your RPO. To calculate RPO, first determine how much data loss is acceptable, according to your BCP. Then, figure out how quickly that data loss might occur, as a time measurement.\n\nFor example, suppose you determine that the data your application generates is important but not critical, so that losing 800 records would be acceptable. You further calculate that even during peak times, no more than 100 records are created in an hour. In this scenario, you decide that an RPO of 8 hours is sufficient to meet your needs. If you then implement a disaster recovery plan that meets this RPO, you are sure to do data backups at least every 8 hours. Then, if a disaster occurs at 22:00, the system should be able to recover all data that was in the system before 14:00 PM.\n\n### 5.5. Recovery Time Objective (RTO)\n\n**Recovery time objective (RTO)** is the maximum acceptable amount of time after a disaster strikes that a business process can remain out of commission.\n\n**How quickly must your applications and data be recovered?**\n\n**Example RTO:** The application can be unavailable for a maximum of 1 hour.\n\nAnother important measure of a disaster recovery plan is to define the RTO. RTO is the time it takes after a disruption to restore your applications and recover your data. To continue the previous example, suppose a disaster occurs at 22:00 and the RTO is 1 hour. In that scenario, the DR process should restore the business process to the acceptable service level by 23:00.\n\nA company typically decides on acceptable RPO and RTO, and bases its decision on the financial impact to the business when systems are unavailable. The company determines financial impact by considering many factors, including loss of business and damage to its reputation because of downtime and the lack of systems availability.\n\nIT organizations then plan solutions to provide cost-effective system recovery. The solutions are based on the RPO within the timeline and the service level that the RTO establishes.\n\n### 5.6. Plan for Disaster Recovery\n\nBe intentional about where your data is stored and where your applications run.\n\nTo properly scope your disaster recovery planning, you must look holistically at your use of AWS. Most organizations use a combination of services that can be broadly categorized as encompassing these five service categories areas:\n\n- Storage\n- Compute\n- Networking\n- Databases\n- Deployment orchestration services\n\nIf a disaster occurs, your RPO and RTO will guide your backup-and-restore plans and procedures across each of these service areas. They will also likely affect your production deployment architecture.\n\nIt is also important to keep in mind that, although it\'s unlikely for a Region to be unavailable, it is within the realm of possibility. If some large-scale event affects a Region—for instance, a meteor strike—would your data still be available? Would your applications still be accessible? AWS provides multiple Regions around the world. Thus, you can choose the most appropriate location for your disaster recovery site, in addition to the site where your system is fully deployed.\n\n### 5.7. Storage and Backup Building Blocks\n\nThe following services are referenced in the diagram:\n\n- Amazon Elastic Block Store (Amazon EBS)\n- Amazon Elastic Compute Cloud (Amazon EC2)\n- Amazon Elastic File System (Amazon EFS)\n- Amazon Simple Storage Service (Amazon S3)\n- Amazon Simple Storage Service Glacier (Amazon S3 Glacier)\n\nTo start your disaster planning in detail, look at the data storage layer (postponing the discussion of the database layer for the moment).\n\nYour AWS Cloud storage can consist of a combination of block storage, file system storage, and object storage. Meanwhile, your organization might also use AWS services that connect the on-premises data center to the AWS Cloud.\n\nIn the next few slides, you will learn about high-level best practices for each of these three areas.\n\nOne service that you might not be familiar with is AWS DataSync. AWS DataSync provides movement of large amounts of data online between on-premises storage and Amazon S3, Amazon EFS, or Amazon FSx for Windows File Server. It supports scripted copy jobs and scheduled data transfers from on-premises Network File Systems (NFS) and Server Message Block (SMB) storage. It can also optionally use AWS Direct Connect links.\n\n### 5.8. Best Practice: S3 Cross-Region Replication\n\nMost S3 storage classes replicate data across Availability Zones within a single Region.\n\n- Configure S3 cross-Region replication for higher-level data security.\n- Automatically, asynchronously replicates objects created after you add the replication configuration.\n- Can also help meet compliance requirements and reduce latency for users who are accessing objects.\n\nFor many organizations, the bulk of their data that is stored on AWS is in Amazon S3, which provides object storage.\n\nRecall that S3 buckets exist in a specific AWS Region. You choose the Region when you create the bucket. Amazon S3 provides 11 9s (99.999999999 percent) of durability for S3 Standard, S3 Standard-IA, S3 One Zone-IA, and Amazon S3 Glacier storage classes. Amazon S3 Standard, S3 Standard-IA, and Amazon S3 Glacier are all designed to sustain data if an entire Amazon S3 Availability Zone loss occurs. They provide this stability by automatically storing your objects across a minimum of three Availability Zones, each separated miles apart, across a single AWS Region.\n\nFor critical applications and data scenarios where you want a higher level of data security, it is a best practice to configure S3 cross-Region replication. To enable the replication, you add a replication configuration to your source bucket. The minimum configuration must indicate the destination bucket where you want Amazon S3 to replicate all objects, or a subset of all objects. It must also include an AWS Identity and Access Management (IAM) role that grants Amazon S3 permissions to copy the objects to the destination bucket.\n\nCopied objects retain their metadata. The destination bucket can belong to another storage class. For example, the contents of an S3 Standard bucket might be replicated to an Amazon S3 Glacier bucket. You can assign different ownership to the objects in the destination bucket. You can also use S3 Replication Time Control (S3 RTC) to replicate your data across different Regions in a predictable time frame. S3 RTC replicates four 9s (99.99 percent) of new objects stored in Amazon S3 within 15 minutes (backed by a service-level agreement).\n\n### 5.9. Best Practice: EBS Volume Snapshots\n\nCreate point-in-time snapshots of EBS volumes.\n\n- Snapshots provide incremental backups (they back up the blocks that changed since the previous snapshot).\n- Snapshots enable you to restore data to a new EBS volume.\n- Use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of snapshots.\n- You cannot snapshot instance storage.\n\nRegarding block storage, you can back up the data that is on EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that it saves only the blocks on the device that have changed since your most recent snapshot. This architecture minimizes the time that is required to create the snapshot, and it saves on storage costs by not duplicating data.\n\nEach snapshot contains all the information that is needed to restore your data (from the moment when the snapshot was taken) to a new EBS volume. When you create an EBS volume that is based on a snapshot, the new volume begins as an exact replica of the original volume. This original volume was used to create the snapshot. The replicated volume loads data in the background so that you can begin to use it immediately. If you access data that has not been loaded yet, the volume immediately downloads the requested data from Amazon S3. Then, it continues to load the rest of the volume\'s data in the background.\n\nAmazon EBS volumes provide off-instance storage that persists independently from the life of an instance, and is replicated across multiple servers in an Availability Zone. Volumes prevent the loss of data from the failure of any single component. After you create a snapshot, it finishes copying to Amazon S3 (when the snapshot status is completed). Then, you can copy it from one AWS Region to another, or within the same Region.\n\nYou can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of snapshots that back up your EBS volumes. Automating snapshot management helps you to:\n\n- Protect valuable data by enforcing a regular backup schedule.\n- Retain backups as required by auditors or internal compliance.\n- Reduce storage costs by deleting outdated backups.\n\nYou cannot create snapshots of EC2 instance store volumes. However, if you must back up data from an instance store, you can create a new EBS volume and format it. Then, mount the new volume to the EC2 instance guest OS and copy the data on your instance store volume to the EBS volume.\n\nRecall that instance store volumes provide temporary block-level storage that works well for information that changes frequently, such as buffers, caches, and scratch data. You might find that you must back up data from an instance store. If so, you might want to rethink why you are storing that data on an instance store volume in the first place.\n\n### 5.10. Best Practice: File System Replication\n\nIt is also a best practice to replicate your file storage.\n\nAWS DataSync makes data move faster between two EFS or Amazon FSx Windows File Server file systems, or between on-premises storage and AWS file storage. You can use DataSync to transfer datasets over DX or the internet. Use the service for one-time data migrations or ongoing workflows for data protection and recovery.\n\nYou can learn more about how to use AWS Backup to manage EBS volume backups and to automate backups of EFS file systems. See the Scheduling automated backups using Amazon EFS and AWS Backup blog for details.\n\nFSx for Windows File Server takes daily automatic backups of your file systems, and it enables you to take more backups at any point. Amazon FSx stores the backups in Amazon S3. The daily backup window is a 30-minute window that you specify when you create a file system. The daily backup retention period that is specified for your file system determines the number of days that your daily automatic backups are kept. (This number is 7 days by default.)\n\nLike most Amazon S3 storage classes replicate data across Availability Zones, so do Amazon EFS and FSx for Windows File Server file systems. Your disaster recovery requirements might specify that you need a multi-region recovery solution. In that case, it is a best practice to replicate your Amazon EFS and FSx for Windows File Server file systems to a second Region. You can use AWS DataSync to get this replication. To simplify file transfer between two EFS file systems by using DataSync, you can use the AWS DataSync In-Cloud QuickStart and Scheduler.\n\n### 5.11. Compute Capacity Should Be Quickly Recoverable\n\nObtain and boot new server instances or containers within minutes.\n\nIn the context of DR, it\'s critical that you can rapidly create virtual machines that you control. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location.\n\nYou can arrange for automatic recovery of an EC2 instance when a system status check of the underlying hardware fails. The instance is rebooted (on new hardware, if necessary)—but it retains its instance ID, IP addresses, EBS volume attachments, and other configuration details. For a complete recovery, make sure that the instance is configured to automatically start up any services or applications as part of its initialization process.\n\nAmazon Machine Images (AMIs) are preconfigured with operating systems, and some preconfigured AMIs might also include application stacks. You can also configure your own custom AMIs. In the context of DR, AWS recommends that you configure and identify your own AMIs so that they launch as part of your recovery procedure. Such AMIs should be preconfigured with your operating system of choice, in addition to the appropriate pieces of the application stack.\n\n### 5.12. Strategies for Compute Disaster Recovery\n\n- Use the Amazon EC2 snapshot capability for backups.\n- Snapshots can be performed manually, or scheduled (for example, by using AWS Lambda).\n- Use system or instance level system backups infrequently and as a last resort.\n- Drives up the cost of storage that is used quickly.\n- Prefer automated rebuild from configuration or code repositories instead.\n- Cross-region AMI copies.\n- Cross-region snapshot copies.\n- Consider transient compute architectures.\n- Store essential data off the instance.\n\nFor disaster recovery of compute resources, you will probably want to use the Amazon EC2 snapshot capability. Snapshots can be performed manually, or they can be scheduled.\n\nAlthough you can create system or instance-level system backups, extensive use of this approach increases your storage costs. A better approach is to configure an automated rebuild process, where your source code is stored in a repository.\n\nYou might want to replicate Amazon S3 across Regions, and you probably also want to replicate your most critical AMIs and snapshots across Regions.\n\nFinally, consider architecting your use of compute resources to store essential data off of the instances. As you see in the example, your data can be stored in an S3 bucket. When you must do data processing, you can launch one or more EC2 instances from a custom AMI that is preconfigured with application software. As soon as the instance is started, it can pull the needed data from the S3 bucket and process the data. Then, it can write the output data back to Amazon S3 (perhaps to another S3 bucket). After the instance completes its compute tasks, the instance can be terminated. Such an architecture—when it can still meet your business needs—makes it easier to design your disaster recovery strategy. It also can save on costs, because servers that are not in constant use can be terminated and then later re-created when needed.\n\n### 5.13. Networking: Design for Resilience, Recovery\n\n- **Amazon Route 53:**\n    - Traffic distribution.\n    - Failover.\n- **Elastic Load Balancing:**\n    - Load balancing.\n    - Health checks and failover.\n- **Amazon Virtual Private Cloud (Amazon VPC):**\n    - Extend your existing on-premises network topology to the cloud.\n- **AWS Direct Connect:**\n    - Fast, consistent replication and backups of large on-premises environments to the cloud.\n\nWhen you work to recover from a disaster, it\'s likely that you must modify network settings to fail your system over to another site. AWS offers several services and features that enable you to manage and modify network settings, a few of which are highlighted next.\n\nAmazon Route 53 provides load balancing and network routing capabilities that enable you to distribute network traffic. It also provides the ability to fail over between multiple endpoints and even to a static website that is hosted in Amazon S3.\n\nThe Elastic Load Balancing service automatically distributes incoming application traffic across multiple EC2 instances. It enables you to achieve fault tolerance in your applications by providing the load-balancing capacity that is needed in response to incoming application traffic. You can pre-allocate a load balancer so that its Domain Name System (DNS) name is already known, which can simplify implementation of your DR plan.\n\nYou can use Amazon Virtual Private Cloud (Amazon VPC) to extend an existing on-premises network topology to the cloud. This extension can be especially appropriate when you recover enterprise applications that might be hosted on an internal network.\n\nFinally, AWS Direct Connect simplifies the setup of a dedicated network connection from an on-premises data center to AWS. Using DX can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections.\n\n### 5.14. Databases: Features That Support Recovery\n\n- **Amazon Relational Database Service (Amazon RDS):**\n    - Take snapshot data and save it in a separate Region.\n    - Combine read replicas with Multi-AZ deployments to build a resilient disaster recovery strategy.\n    - Retain automated backups.\n- **Amazon DynamoDB:**\n    - Back up entire tables in seconds.\n    - Use point-in-time-recovery to continuously back up tables for up to 35 days.\n    - Initiate backups with a single click in the console or a single application programming interface (API) call.\n    - Use Global Tables to build a multi-region, multi-master database that provides fast local performance for massively scaled globally distributed applications.\n\nAWS provides many database services. Some key features of Amazon RDS and Amazon DynamoDB that are relevant to disaster recovery scenarios are explained next.\n\nConsider using Amazon RDS in the DR preparation phase to store a copy of your critical data in a database that is already running. Then, use Amazon RDS in the DR recovery phase to run your production database.\n\nIf you implement a multi-region DR plan, Amazon RDS gives you the ability to store snapshot data that was captured from one Region to another Region. You can share a manual snapshot with up to 20 other AWS accounts.\n\nCombining read replicas with Multi-AZ deployments enables you to build a resilient disaster recovery strategy and simplify your database engine upgrade process. By using Amazon RDS read replicas, you can create one or more read-only copies of your database instance. You can create these copies within the same AWS Region, or in a different AWS Region. Updates to the source database are then asynchronously copied to your read replicas. Read replicas can be promoted to become a standalone database instance, when needed.\n\nUse Amazon DynamoDB in the preparation phase to copy data to DynamoDB in another Region or to Amazon S3. During the recovery phase of DR, you can scale up in minutes. DynamoDB global tables replicate your DynamoDB tables automatically across your choice of AWS Regions. They resolve update conflicts and enable your applications to stay highly available, even in the unlikely event that an entire Region is isolated or affected by degradation.\n\n### 5.15. Automation Services: Quickly Replicate or Redeploy Environments\n\n- **AWS CloudFormation:**\n    - Use templates to quickly deploy collections of resources as needed.\n    - Duplicate production environments in a new Region or VPC in minutes.\n- **AWS Elastic Beanstalk:**\n    - Quickly redeploy your entire stack in only a few clicks.\n- **AWS OpsWorks:**\n    - Automatic host replacement.\n    - Combine it with AWS CloudFormation in the recovery phase.\n    - Provision a new stack that supports the defined RTO.\n\nWhen you use automation services, you can quickly replicate or redeploy environments.\n\nAWS CloudFormation enables you to model and deploy your entire infrastructure in a text file. This template can become the single source of truth for your infrastructure. When you use AWS CloudFormation to manage your entire infrastructure, it also becomes a powerful tool in your disaster recovery planning toolkit. It enables you to duplicate complex production environments in minutes, for example, to a new Region or a new VPC.\n\nAWS CloudFormation provisions your resources in a repeatable manner, which enables you to build and rebuild your infrastructure and applications. You are not required to perform manual actions or write custom scripts.\n\nIf you use AWS Elastic Beanstalk to host your applications, you can upload an updated application source bundle and deploy it to your AWS Elastic Beanstalk environment. Alternatively, you can redeploy a previously uploaded version of an application. You can also deploy a previously uploaded version of your application to any of its environments.\n\nFinally, AWS OpsWorks is an application management service that makes it easy to deploy and operate applications of all types and sizes. You can define your environment as a series of layers, and configure each layer as a tier of your application. AWS OpsWorks has automatic host replacement, so if you have an instance failure, it is automatically replaced. You can use AWS OpsWorks in the DR preparation phase to template your environment and combine it with AWS CloudFormation in the DR recovery phase.\n\n## 6. Section 2 Key Takeaways\n\n- To choose the correct disaster recovery strategy, first identify your recovery point objective (RPO) and recovery time objective (RTO).\n- Use features such as S3 Cross-Region Replication, EBS volume snapshots, and Amazon RDS snapshots to protect data.\n- Use networking features (such as Route 53 failover and Elastic Load Balancing) to improve application availability.\n- Use automation services (such as AWS CloudFormation) as part of your DR strategy to quickly deploy duplicate environments when needed.\n\nSome key takeaways from this section of the module include:\n\n- To choose the correct disaster recovery strategy, first identify your recovery point objective (RPO) and recovery time objective (RTO).\n- Use features such as S3 Cross-Region Replication, EBS volume snapshots, and RDS snapshots to protect data.\n- Use networking features—such as Route 53 failover and Elastic Load Balancing—to improve application availability.\n- Use automation services—such as AWS CloudFormation—as part of your DR strategy to quickly deploy duplicate environments when necessary.\n\n## 7. Section 3: Disaster Recovery Patterns\n\n### 7.1. Common Disaster Recovery Patterns on AWS\n\nFour disaster recovery patterns:\n\n- Backup and restore.\n- Pilot light.\n- Warm standby.\n- Multi-site.\n\nEach pattern is suited to a different combination of:\n\n- Recovery point objective.\n- Recovery time objective.\n- Cost-effectiveness.\n\nOrganizations often use these four common disaster recovery patterns:\n\n- Backup and restore.\n- Pilot light.\n- Warm standby.\n- Multi-site.\n\nAs you will discover in the details that follow, each pattern is well-suited to different requirements. Some of the patterns offer better cost-effectiveness. Others provide a faster RPO and faster RTO, but cost more to maintain.\n\n### 7.2. Backup and Restore Pattern\n\nBack up configuration and state data to S3. Implement a lifecycle policy to save on cost.\n\nThe first disaster recovery approach is the backup and restore pattern.\n\nIn most traditional environments, data is backed up to tape and sent offsite regularly. If you use this method, it can take a long time to restore your system when a disaster occurs.\n\nAmazon S3 provides a more easily accessible destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location.\n\nIn the example backup scenario, data is copied from the on-premises data center to Amazon S3. AWS DataSync or Amazon S3 Transfer Acceleration can optionally be used as part of this configuration to automate or increase the speed of data transfer. Then, an S3 lifecycle configuration that is applied to the bucket later moves the backup data to less-expensive Amazon S3 storage classes. The backup data moves to Amazon S3 Glacier or Amazon S3 Standard-IA, which saves on cost as the data ages and is not frequently accessed.\n\nIn the example restore scenario, the on-premises data might be temporarily or permanently lost. Then, the backup data can be downloaded from Amazon S3 back to the on-premises servers.\n\nIf your corporate data center remains offline, you can further ensure the ability to restore your data to your servers. You can have Amazon EC2 servers that are ready to go in a VPC in your designated disaster recovery Region. This Region can connect to the S3 bucket that contains your backup application data. It can read that data, and perhaps temporarily host your applications while you work to restore your data center.\n\n### 7.3. AWS Storage Gateway\n\nAs part of the backup and restore pattern, you might find that it makes sense to use AWS Storage Gateway.\n\nAWS Storage Gateway is a hybrid storage service that enables your on-premises applications to use AWS Cloud storage. You can use the service for backup and archiving, disaster recovery, cloud data processing, storage tiering, and migration.\n\nYour applications connect to the service through a virtual machine or hardware gateway appliance by using standard storage protocols. These protocols include NFS, SMB, Virtual Tape Library (VTL), and Internet Small Computer System Interface (iSCSI). The gateway connects to AWS storage services—such as Amazon S3, Amazon S3 Glacier, and Amazon EBS—which provide storage for files, volumes, and virtual tapes. The service includes an optimized data transfer mechanism. It provides bandwidth management, automated network resilience, and efficient data transfer, in addition to a local cache for low-latency on-premises access to your most active data.\n\nWith a file gateway, you store and retrieve objects (by using the NFS or SMB protocol) in Amazon S3. You use a local cache for low-latency access to your most recently used data. When your files are transferred to Amazon S3, they are stored as objects and can be accessed through an NFS mount point.\n\nThe Storage Gateway volume interface presents your applications with block storage disk volumes that can be accessed by using the iSCSI protocol. Data on these volumes is backed up as point-in-time EBS snapshots, which enables you to access it through Amazon EC2, if needed.\n\nThe Storage Gateway tape interface presents the Storage Gateway to your existing backup application as a virtual tape library. This library consists of a virtual media changer and virtual tape drives. You can continue to use your existing backup applications while you write to a collection of virtual tapes. Each virtual tape is stored in Amazon S3. When you no longer require access to data on virtual tapes, your backup application archives it from the virtual tape library into Amazon S3 Glacier.\n\n### 7.4. Backup and Restore: Checklist\n\n#### 7.4.1. Preparation Phase\n\n- Create backups of current systems.\n- Store backups in Amazon S3.\n- Document the procedure to restore from backups.\n- Know:\n    - Which AMI to use, and build as needed.\n    - How to restore the system from backups.\n    - How to route traffic to the new system.\n    - How to configure the deployment.\n\n#### 7.4.2. In Case of Disaster\n\n- Retrieve backups from Amazon S3.\n- Restore required infrastructure:\n    - EC2 instances from prepared AMIS.\n    - Elastic Load Balancing load balancers.\n    - AWS resources created by an AWS CloudFormation stack—automated deployment to restore or duplicate the environment.\n- Restore the system from backup.\n- Route traffic to the new system.\n- Adjust Domain Name System (DNS) records accordingly.\n\nIf you implement the backup and restore disaster recovery pattern, the key steps that you should complete during the preparation phase are:\n\n- Create backups of current systems.\n- Store backups in Amazon S3.\n- Document the procedure to restore from backups.\n\nIf you implement this pattern, the key steps to complete in case of disaster are:\n\n- Retrieve backups from Amazon S3.\n- Start the required infrastructure.\n- Restore the system from backups.\n- Finally, route traffic to the new system.\n\n### 7.5. Pilot Light Pattern: Preparation Phase\n\nThe second disaster recovery approach is the pilot light pattern.\n\nPilot light describes a disaster recovery pattern where a minimal backup version of your environment is always running. The pilot light analogy comes from a gas heater: a small flame (or the pilot light) is always on, even when the heater is off. The pilot light can quickly ignite the entire furnace to heat a house. In the example pattern, the pilot light is the secondary database that is always running.\n\nThe pilot light scenario is similar to the backup-and-restore scenario. However, recovery time is typically faster because the core pieces of the system are already running and are continually kept up-to-date. When the time comes for recovery, you can rapidly provision a full production environment around the critical core.\n\nInfrastructure elements for the pilot light itself typically include your database servers. This grouping is the critical core of the system (the pilot light). All other infrastructure pieces can quickly be provisioned around it to restore the complete system. To provision the rest of the infrastructure, you typically bundle preconfigured servers as AMIs that are ready to be started at a moment\'s notice. (Or they might be instances that are in a stopped state.) When recovery begins, these instances start quickly with their pre-defined role, which enables them to connect to the database.\n\nThis pattern is relatively inexpensive to implement. Regularly changing data must be replicated to the pilot light, the small core around which the full environment starts in the recovery phase. Your less frequently updated data, such as operating systems and applications, can be periodically updated and stored as AMIs.\n\n### 7.6. Pilot Light Pattern: In Case of Disaster\n\nSuppose that disaster strikes, and your primary application goes offline. In this case, you can quickly commission the compute resources to run the application or to orchestrate the failover to pilot light resources in AWS. In this example, the secondary database stores critical data. If there is a disaster, the new web server and app server start up and connect to the secondary database. Amazon Route 53 is configured to then route traffic to the new web server.\n\nThe primary environment can exist in an on-premises data center, or in another Region or Availability Zone on AWS. Either way, you can use the pilot light pattern to meet your recovery time objective (RTO).\n\n### 7.7. Pilot Light Pattern: Checklist\n\n#### 7.7.1. Preparation Phase\n\n- Configure EC2 instances to replicate or mirror servers.\n- Ensure that all supporting custom software packages are available.\n- Create and maintain AMIs of key servers where fast recovery is needed.\n- Regularly run these servers, test them, and apply any software updates and configuration changes.\n- Consider automating the provisioning of AWS resources.\n\n#### 7.7.2. In Case of Disaster\n\n- Automatically bring up resources around the replicated core dataset.\n- Scale the system as needed to handle current production traffic.\n- Switch over to the new system.\n- Adjust DNS records to point to AWS.\n\nIf you implement the pilot light disaster recovery pattern, the key steps that you should complete during the preparation phase are:\n\n- Configure the EC2 instances.\n- Ensure that all of the supporting custom software packages are available.\n- Create and maintain essential AMIs where fast recovery is required.\n- Regularly run and test servers, and apply software updates and configuration updates.\n- Consider automating the provisioning of AWS resources.\n\nIf you implement the pilot light pattern, the key steps to complete in case of disaster are:\n\n- Automatically bring up resources around the replicated core dataset.\n- Scale the system as needed to handle current production traffic.\n- Switch over to the new system by adjusting the DNS records to point to the backup deployment.\n\n### 7.8. Warm Standby Pattern: Preparation Phase\n\nThe third disaster recovery approach is the warm standby pattern.\n\nThe warm standby pattern is like the pilot light, but more resources are already running. The term warm standby describes a disaster recovery scenario where a scaled-down version of a fully functional environment is always running in the cloud. The warm standby solution extends the pilot light elements and preparation. It further decreases the recovery time because some services are always running. By identifying your business-critical systems, you can fully duplicate these systems and have them always on.\n\nThese servers can be running on a minimum-sized fleet of EC2 instances with the smallest sizes possible. This solution is not yet scaled to take a full production load, but it is fully functional. Though it exists for DR purposes, you can also use it for non-production work, such as testing, quality assurance, and internal use.\n\nIn the example, two systems are running. The main system might be running in an on-premises data center or an AWS Region, and a low-capacity system is running on AWS. Use Amazon Route 53 to distribute requests between the main system and the backup system.\n\n### 7.9. Warm Standby Pattern: In Case of Disaster\n\nIn a disaster, if the primary environment is unavailable, Amazon Route 53 switches over to the secondary system.\n\nThe secondary', "# AWS Academy Cloud Architecting\n## Module 13: Building Microservices and Serverless Architectures\n\n### Contents\n\n- **Module 13: Building Microservices and Serverless Architectures** 4\n\n### Module overview\n\n**Sections**\n\n1. Architectural need\n2. Introducing microservices\n3. Building microservice applications with AWS container services\n4. Introducing serverless architectures\n5. Building serverless architectures with AWS Lambda\n6. Extending serverless architectures with Amazon API Gateway\n7. Orchestrating microservices with AWS Step Functions\n\n**Demonstrations**\n\n- Creating an AWS Lambda function\n- Using AWS Lambda with Amazon S3\n\n**Labs**\n\n- (Optional) Guided Lab 1: Breaking a Monolithic Node.js Application into Microservices\n- Guided Lab 2: Implementing a Serverless Architecture on AWS\n- Challenge Lab: Implementing a Serverless Architecture for the Café\n\n**Knowledge check**\n\n**This module includes the following sections:**\n\n- Architectural need\n- Introducing microservices\n- Building microservice applications with AWS container services\n- Introducing serverless architectures\n- Building serverless architectures with AWS Lambda\n- Extending serverless architectures with Amazon API Gateway\n- Orchestrating microservices with AWS Step Functions\n\n**This module also includes:**\n\n- Two AWS Lambda demonstrations\n- An optional guided lab where you refactor a monolithic application into microservices\n- A guided lab where you implement a serverless architecture on AWS with Amazon S3, AWS Lambda, Amazon DynamoDB, and Amazon SNS\n- A challenge lab where you use AWS Lambda and Amazon Simple Notification Service (Amazon SNS) to generate and send a daily sales report for the café.\n\nFinally, you will be asked to complete a knowledge check that will test your understanding of key concepts covered in this module.\n\n### Module objectives\n\nAt the end of this module, you should be able to:\n\n- Indicate the characteristics of microservices\n- Refactor a monolithic application into microservices and use Amazon ECS to deploy the containerized microservices\n- Explain serverless architecture\n- Implement a serverless architecture with AWS Lambda\n- Describe a common architecture for Amazon API Gateway\n- Describe the types of workflows that AWS Step Functions supports\n\n### Section 1: Architectural need\n\n#### Introducing Section 1: Architectural need. \n\n#### Café business requirement\n\nThe café wants to get daily reports via email about all the orders that were placed on the website. They want this information so they can anticipate demand and bake the correct number of desserts going forward (reducing waste). They also want to identify any patterns in their business (analytics).\n\nFrank and Martha want to get daily reports via email about all the orders that were placed on the website. Frank wants to anticipate demand so he can bake the correct number of desserts going forward (reducing waste). Martha wants to identify any patterns in the café's business (analytics). Currently, Sofía has set up a cron job on the web server instance that sends these daily order report email messages to Frank and Martha. However, the cron job is resource-intensive and reduces web server performance.\n\nOlivia advises Sofía and Nikhil that non-business-critical reporting tasks should be kept separate. Sofía and Nikhil want to further decouple the architecture and move the cron job into a managed, serverless environment that will scale well and reduce costs.\n\n### Section 2: Introducing microservices\n\n#### Introducing Section 2: Introducing microservices.\n\n#### What are microservices?\n\nApplications that are composed of independent services that communicate over well-defined APIs\n\nMicroservices are an architectural and organizational approach to software development where applications are composed of independent services that communicate over well-defined application programming interfaces (APIs). This approach is designed to speed up deployment cycles.\n\nThe microservices approach fosters innovation and ownership, and improves the maintainability and scalability of software applications.\n\n#### Monolithic versus microservice applications\n\nTo understand the benefits of microservices, consider first a monolithic application.\n\nIn the example on the left, the three processes (users, topics, and messages) of a monolithic forum application are tightly coupled. They run as a single service. If one process of the application experiences a spike in demand, the entire architecture must be scaled. Adding or improving features becomes more complex as the code base grows, which limits experimentation and makes it difficult to implement new ideas. The availability of monolithic applications is also at risk because many dependent and tightly coupled processes increase the impact of a single process failure.\n\nNow, suppose that the same application runs in a microservice architecture. Each process of the application is built as an independent component that runs as a service. The services communicate by using lightweight API operations. Each service performs a single function that can support multiple applications. Because the services run independently, they can be updated, deployed, and scaled to meet the demand for specific functions of an application.\n\nA microservice architecture provides much quicker iteration, automation, and overall agility. Start fast, fail fast, and recover fast.\n\nFor an overview of microservices on AWS, see What are Microservices?\n\n#### Characteristics of microservices\n\nMicroservices share some common characteristics:\n\n- Decentralized – Microservice architectures are distributed systems with decentralized data management. They don't rely on a unifying schema in a central database. Each microservice has its own view about data models. Microservices are also decentralized in the way they are developed, deployed, managed, and operated.\n- Independent – Each component service in a microservice architecture can be changed, upgraded, or replaced independently without affecting the function of other services. Services do not need to share any of their code or implementation with other services. Similarly, the teams responsible for different microservices can act independently from each other.\n- Specialized - Each component service is designed for a set of capabilities and focuses on a specific domain. If the code for a particular component service reaches a certain level of complexity, then the service can be split into two or more services.\n- Polyglot - Microservices don't follow a single approach. Teams have the freedom to choose the best tool for their specific problem. As a consequence, microservice architectures take a heterogeneous approach to operating systems, programming languages, data stores, and tools. This approach is called polyglot persistence and programming.\n- Black boxes - Individual component services are designed as black boxes, which mean that the details of their complexity are hidden from other components. Any communication between services happens through well-defined APIs to prevent implicit and hidden dependencies.\n- You build it, you run it – DevOps is a key organizational principle for microservices, where the team responsible for building a service is also responsible for operating and maintaining it in production.\n\n#### Section 2 key takeaways\n\nSome key takeaways from this section of the module include:\n\n- Microservice applications are composed of independent services that communicate over well-defined APIS\n- Microservices share the following characteristics\n    - Decentralized: Microservices are decentralized in the way they are developed, deployed, managed, and operated\n    - Independent: Each component service in a microservices architecture can be developed, deployed, operated, and scaled without affecting the function of other services\n    - Specialized: Each component service is designed for a set of capabilities and focuses on solving a specific problem\n    - Polyglot: Microservice architectures take a heterogeneous approach to operating systems, programming languages, data stores, and tools\n    - Black boxes: The details of the complexity of microservice components are hidden from other components\n    - You build it, you run it: DevOps is a key organizational principle for microservices\n\n### Section 3: Building microservice applications with AWS container services\n\n#### Introducing Section 3: Building microservice applications with AWS container services.\n\n#### What is a container?\n\nWhen you build a microservice architecture, you can use containers for the processing power.\n\nContainers are a method of operating system virtualization that enables you to run an application and its dependencies in resource-isolated processes. A container is a lightweight, standalone software package. It contains everything that a software application needs to run, such as the application code, runtime engine, system tools, system libraries, and configurations.\n\n#### A problem that containers solve\n\nContainers can help ensure that applications deploy quickly, reliably, and consistently, regardless of deployment environment. Containers also give you more granular control over resources, which improves the efficiency of your infrastructure.\n\n#### Container terminology\n\nA container is created from a read-only template that is called an image. Images are typically built from a Dockerfile, which is a plaintext file that specifies all the components that are included in the container. You can create images from scratch, or you can use images that others created and published to a public or private container registry.\n\nA container image is the snapshot of the file system that is available to the container. For example, you might have the Debian operating system as a container image. When you run this container, a Debian operating system is available to it. You can also package all your code dependencies in the container image and use it as your code artifact.\n\nContainer images are stored in a registry. You can download the images from the registry and run them on your cluster. Registries can exist in or outside your AWS infrastructure.\n\n#### Amazon ECS\n\nYou can run your containers on Amazon Elastic Container Service (Amazon ECS). Amazon ECS is a highly scalable, high-performance, container-management service. It supports Docker containers and enables you to easily run applications on a managed cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances.\n\nAmazon ECS is a scalable cluster service for hosting containers that:\n\n- Can scale up to thousands of Docker containers in seconds\n- Monitors container deployment\n- Manages the state of the cluster that runs the containers\n- Schedules containers by using a built-in scheduler or third-party scheduler (Apache Mesos, Blox)\n- Is extensible by using APIs\n- Can be launched with either AWS Fargate or Amazon EC2 launch types\n\nYou can run ECS clusters at scale by mixing Spot Instances with On-Demand Instances and Reserved Instances.\n\n#### Amazon ECS orchestrates containers\n\nAmazon ECS is a regional service that simplifies running application containers in a highly available manner across multiple Availability Zones within a Region. You can create ECS clusters in a new or existing virtual private cloud (VPC). A cluster is a logical grouping of resources.\n\nAfter a cluster is up and running, you can define task definitions and services that specify which Docker container images to run across your clusters.\n\nA task definition is a text file in JavaScript Object Notation (JSON) format. It describes one or more containers, up to a maximum of 10, that form your application. You can think of it as a blueprint for your application. Task definitions specify parameters for your application-for example, which containers and launch type to use. Other parameters include which ports should be opened for your application and what data volumes should be used with the containers in the task.\n\nA service enables you to specify how many copies of your task definition to run and maintain in a cluster. You can optionally use an Elastic Load Balancing load balancer to distribute incoming traffic to containers in your service. Amazon ECS maintains that number of tasks and coordinates task scheduling with the load balancer.\n\nAfter you create a task definition for your application, you can specify the number of tasks that will run on your cluster. A task is the instantiation of a task definition within a cluster. When you use Amazon ECS to run tasks, you place them in a cluster. Amazon ECS downloads your container images from a registry that you specify, and runs those images within your cluster.\n\n#### Amazon ECS launch types\n\nAmazon ECS offers two launch types for hosting your containerized applications.\n\nYou can use the Fargate launch type to host your cluster on a serverless infrastructure that Amazon ECS manages. You only need to package your application in containers, specify the CPU and memory requirements, define networking and AWS Identity and Access Management (IAM) policies, and launch the application.\n\nAlternatively, if you want more control, you can use the EC2 launch type to host your tasks on a cluster of EC2 container instances that you manage. A container instance is an EC2 instance that is running the Amazon ECS container agent. You can use Amazon ECS to schedule the placement of containers across your cluster based on your resource needs, isolation policies, and availability requirements. For information about different scheduling options, see Scheduling Amazon ECS Tasks. Amazon ECS keeps track of all the CPU, memory, and other resources in your cluster. It also finds the best server for a container to run on based on your specified resource requirements.\n\nFor more information about the Fargate and EC2 launch types, see Amazon ECS Launch Types.\n\n#### Amazon ECS cluster auto scaling\n\nYou can create an Auto Scaling group for an Amazon ECS cluster. The Auto Scaling group contains container instances that you can scale out (and in) by using Amazon CloudWatch alarms. If you configure your Auto Scaling group to remove container instances, any tasks that are running on the removed container instances are stopped. If your tasks are running as part of a service, Amazon ECS restarts those tasks on another instance if the required resources are available. Examples of such required resources include CPU, memory, ports. However, tasks that were started manually are not restarted automatically.\n\nYou can also take advantage of Amazon ECS cluster auto scaling, which gives you more control over how you scale tasks in a cluster. It increases the speed and reliability of cluster scale-out. It gives you control over the amount of spare capacity that is maintained in your cluster, and automatically manages instance termination on scale-in.\n\nWith cluster auto scaling, you can configure Amazon ECS to scale your Auto Scaling group in and out automatically. Cluster auto scaling relies on capacity providers, which link your ECS cluster to the Auto Scaling groups that you want to use. Each Auto Scaling group is associated with a capacity provider, and each capacity provider has only one Auto Scaling group. However, many capacity providers can be associated with one ECS cluster. To scale the entire cluster automatically, each capacity provider manages the scaling of its associated Auto Scaling group.\n\nFor more information about cluster auto scaling, see the Amazon ECS Cluster Auto Scaling AWS News Blog post.\n\n#### Decomposing monoliths – Step 1: Create container images\n\nAgain, consider the monolithic forum application that you saw earlier where the entire application runs as a single service. To rearchitect this application by using a microservice architecture, you can run each application process as a separate service within its own container. With a microservice architecture, the services can scale and be updated independently of the others.\n\nTo deploy the monolithic application as a microservice application, first build and tag an image for each service. Then, register the images with Amazon Elastic Container Registry (Amazon ECR).\n\n#### Decomposing monoliths – Step 2: Create service task definition and target groups\n\nNext, choose a launch type and create a new service for each piece of the original monolithic application. Amazon ECS deploys each service into its own container across an ECS cluster. Then, create a target group for each service. The target group tracks the instances and ports of each container that is running for that service.\n\n#### Decomposing monoliths – Step 3: Connect load balancer to services\n\nFinally, create an Application Load Balancer and configure listener rules to connect to the services. The listener checks for incoming connection requests to your load balancer and uses the rules to route traffic appropriately. In the example, the listener for the Application Load Balancer listens for HTTP service requests on Port 80 and routes them to the appropriate service.\n\n#### Tools for building highly available microservice architectures\n\nAWS Cloud Map and AWS App Mesh are two tools that can help you build highly available microservice architectures.\n\n**AWS Cloud Map**\n\n- Is a fully managed discovery service for cloud resources\n- Can be used to define custom names for application resources\n- Maintains updated location of dynamically changing resources, which increases application availability\n\n**AWS App Mesh**\n\n- Captures metrics, logs, and traces from all your microservices\n- Enables you to export this data to Amazon CloudWatch, AWS X-Ray, and compatible AWS Partner Network (APN) Partner and community tools\n- Enables you to control traffic flows between microservices to help ensure that services are highly available\n\nAWS Cloud Map is a fully managed discovery service for cloud resources. You can use it to define custom names for your application resources (such as databases, queues, microservices, and other cloud resources). AWS Cloud Map maintains the updated location of these dynamically changing resources. This location maintenance increases the availability of your application because your web service always discovers the most up-to-date locations of its resources. You can add and register any resource with minimal manual intervention of mappings. AWS Cloud Map assists with service discovery, continuous integration, and health monitoring of your microservices and applications.\n\nFor more information about AWS Cloud Map, read this AWS Open Source Blog post. To learn more about how you can use AWS Cloud Map to enable your containerized services to discover and connect with each other, read AWS Fargate, Amazon EKS, and Amazon ECS now integrate with AWS Cloud Map.\n\nWhen you create your task definitions, you can enable App Mesh integration. AWS App Mesh captures metrics, logs, and traces from all of your microservices. You can export this data to Amazon CloudWatch, AWS X-Ray, and compatible AWS Partner Network (APN) Partner and community tools for monitoring and tracing. AWS App Mesh also enables you to control how traffic flows between your microservices to make sure that every service is highly available during deployments, after failures, and as your application scales.\n\nApp Mesh enables you to configure microservices to connect directly to each other via a proxy instead of requiring code within the application or by using a load balancer. App Mesh uses Envoy, an open source service-mesh proxy, which is deployed alongside your microservice containers.\n\nFor more information about AWS Cloud Map and AWS App Mesh, see this AWS YouTube video.\n\n#### AWS Fargate\n\n- Is a fully managed container service\n- Works with Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Kubernetes Service (Amazon EKS)\n- Provisions, manages, and scales your container clusters\n- Manages runtime environment\n- Provides automatic scaling\n\nIn this section, you have learned that Amazon ECS offers two launch types: EC2 and Fargate.\n\nAWS Fargate is a fully managed container service that works with both Amazon ECS and Amazon Elastic Kubernetes Service (Amazon EKS). It enables you to run containers without needing to manage servers or clusters. With AWS Fargate, you no longer need to provision, configure, and scale clusters of virtual machines to run containers. As a result, you don't need to choose server types, decide when to scale your clusters, or optimize cluster packing. AWS Fargate reduces the need for you to interact with or think about servers or clusters. Fargate enables you to focus on designing and building your applications instead of managing the infrastructure that runs them.\n\n#### Section 3 key takeaways\n\nSome key takeaways from this section of the module include:\n\n- Amazon ECS is a highly scalable, high-performance container management service. It supports Docker containers and enables you to easily run applications on a managed cluster of Amazon EC2 instances.\n- Cluster auto scaling gives you more control over how you scale tasks within a cluster.\n- AWS Cloud Map enables you to define custom names for your application resources. It maintains the updated location of these dynamically changing resources.\n- AWS App Mesh is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure.\n- AWS Fargate is a fully managed container service that enables you to run containers without needing to manage servers or clusters.\n\n#### Module 13 – Guided Lab 1: Breaking a Monolithic Node.js Application into Microservices \n\n(Optional lab)\n\nYou might choose to complete Module 13 – Guided Lab 1: Breaking a Monolithic Node.js Application into Microservices. This lab is optional.\n\n#### Guided lab 1: Tasks\n\n1. Prepare the AWS Cloud9 development environment\n2. Run a monolithic application on a basic Node.js server\n3. Containerize the monolith for Amazon ECS\n4. Deploy the monolith to Amazon ECS\n5. Refactor the monolith into containerized microservices\n\n#### Guided lab 1: Final product\n\nThe diagram summarizes what you will have built after you complete the lab.\n\n#### Begin Module 13 – Guided Lab 1: Breaking a Monolithic Node.js Application into Microservices\n\nIt is now time to start the optional guided lab.\n\n#### Guided lab 1 debrief: Key takeaways\n\nYour educator might choose to lead a conversation about the key takeaways from this guided lab after you have completed it.\n\n### Section 4: Introducing serverless architectures\n\n#### Introducing Section 4: Introducing serverless architectures.\n\n#### What does serverless mean?\n\nA way for you to build and run applications and services without thinking about servers\n\nSo far, you have learned that you can use Amazon ECS to build your microservice applications by using containers. Amazon ECS is a container orchestration service where you manage your application code, data source integrations, security configuration, updates, network configuration, firewall, and management tasks. You also learned that you can use the Fargate launch type to host your cluster on a serverless infrastructure that Amazon ECS manages.\n\nBut what does serverless mean?\n\nServerless is the native architecture of the cloud that enables you to shift more operational responsibilities to AWS, which can increase your agility and innovation. Serverless enables you to build and run applications and services without thinking about servers. Your application still runs on servers. However, AWS does all the server management tasks, such as server or cluster provisioning, patching, operating system maintenance, and capacity provisioning.\n\n#### Tenets of serverless architectures\n\nThe tenets that define serverless as an operational model include:\n\n- No infrastructure to provision or manage (no servers to provision, operate, or patch)\n- Automatically scales by unit of consumption (scales by unit of work or consumption rather than by server unit)\n- Pay-for-value pricing model (you pay only for the duration that a resource runs, rather than by server unit)\n- Built-in availability and fault tolerance (no need to architect for availability because it is built into the service)\n\nFor more information about what serverless is, see this AWS website.\n\n#### Benefits of serverless\n\nServerless enables you to build modern applications with increased agility and lower total cost of ownership (TCO). By using a serverless architecture, you can focus on your core product. You don't need to worry about managing and operating servers or runtimes, either in the cloud or on premises. This reduced overhead enables you to reclaim time and energy, which you can spend on developing products that scale and are reliable. Finally, serverless architectures enable you to build microservice applications.\n\n#### AWS serverless offerings\n\nAWS has many offerings that you can use to build serverless architectures on AWS. So far in this course, you have already learned about several of them.\n\nThe rest of this module focuses on how you can use AWS Lambda, Amazon API Gateway, and AWS Step Functions to build serverless architectures.\n\n#### Section 4 key takeaways\n\nSome key takeaways from this section of the module include:\n\n- Serverless computing enables you to build and run applications and services without provisioning or managing servers\n- Serverless architectures offer the following benefits\n    - Lower TCO\n    - You can focus on your application\n    - You can use them to build microservice applications\n\n### Section 5: Building serverless architectures with AWS Lambda\n\n#### Introducing Section 5: Building serverless architectures with AWS Lambda.\n\n#### AWS Lambda\n\n- Is a fully managed compute service\n- Runs your code on a schedule or in response to events (for example, changes to an Amazon S3 bucket or an Amazon DynamoDB table)\n- Supports Java, Go, PowerShell, Node.js, C#, Python, Ruby, and Runtime API\n- Can run at edge locations closer to your users\n\nAWS Lambda is a fully managed compute service that runs your code in response to events and automatically manages the underlying compute resources for you. Lambda runs your code on a high-availability compute infrastructure and performs all administration of the compute resources, including server and operating system maintenance, capacity provisioning, automatic scaling, code monitoring, and logging.\n\nAWS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code, and provides a Runtime API that enables you to use any additional programming languages to author your functions.\n\nLambda@Edge is a feature of Amazon CloudFront that enables you to run code closer to users of your application, which improves performance and reduces latency. Lambda@Edge runs your code in response to events that are generated by the Amazon CloudFront content delivery network (CDN). Lambda@Edge enables you to run Node.js and Python Lambda functions to customize content that Amazon CloudFront delivers. For information about how to add HTTP security response headers, read this AWS Networking & Content Delivery Blog post.\n\n#### How AWS Lambda works\n\nAWS Lambda integrates with other AWS services to invoke Lambda functions. A Lambda function is custom code that you write in one of the languages that Lambda supports. You can configure triggers to invoke a function in response to resource lifecycle events, respond to incoming HTTP requests, consume events from a queue, or run on a schedule.\n\nAn event source is the entity that publishes the event to Lambda. Your Lambda function processes the event, and Lambda runs your Lambda function on your behalf.\n\nLambda functions are stateless, which means that they have no affinity to the underlying infrastructure. Lambda can rapidly launch as many copies of the function as needed to scale to the rate of incoming events.\n\n#### Lambda functions\n\nWhen you create a Lambda function, you define the permissions for the function and specify which events trigger the function. You also create a deployment package that includes your application code and any dependencies and libraries that are needed to run your code. Finally, you configure runtime parameters such as memory, time out, and concurrency. When your function is invoked, Lambda will run an environment based on the runtime and configuration options that you selected.\n\nFor more information about how AWS Lambda runs, see the AWS Documentation.\n\n#### Anatomy of a Lambda function\n\nWhen a Lambda function is invoked, the code begins running at the handler. The handler is a specific code method or function that you create and include in your package. You specify the handler when you create a Lambda function. Each supported language has its own requirements for how a function handler can be defined and referenced within the package. After the handler is successfully invoked inside your Lambda function, the runtime environment belongs to the code you wrote.\n\nThe handler always takes two objects: the event object and the context object.\n\nThe event object provides information about the event that triggered the Lambda function. This event might be a pre-defined object that an AWS service generates, or a custom user-defined object in the form of a serializable string. An example of such a string might be a plain old Java object (POJO) or a JSON stream.\n\nThe contents of the event object include all the data and metadata that your Lambda function needs to drive its logic. The contents and structure of the event object vary, depending on which event source created it. For example, an event that is created by API Gateway contains details that are related to the HTTPS request that was made by the API client—such as path, query string, and request body. However, an event that is created by Amazon includes details about the bucket and the new object.\n\nThe context object is generated by AWS and provides metadata about the runtime environment. The context object enables your function code to interact with the Lambda runtime environment. The contents and structure of the context object vary based on the language runtime that your Lambda function uses.\n\nHowever, at a minimum, the context object contains:\n\n- awsRequestId – This property is used to track specific invocations of a Lambda function (important for error reporting or when contacting AWS Support)\n- logStreamName – The CloudWatch log stream that your log statements will be sent to\n- getRemainingTimeInMillis() – This method returns the number of milliseconds that remain before the running of your function times out\n\n#### Lambda function configuration and billing\n\nMemory and timeout are configurations that determine how your Lambda function performs. These configurations affect your billing. With AWS Lambda, you are charged based on the number of requests for your functions (the total number of requests across all your functions) and the duration (the time it takes for your code to run). The price depends on the amount of memory you allocate to your function.\n\n**Memory** - You specify the amount of memory you want to allocate to your Lambda function. Lambda then allocates CPU power that is proportional to the memory. Lambda is priced so that the cost per 1 ms of function duration increases as the memory configuration increases. For example, say that you have a Lambda function with 256 MB of memory, and that it runs for 110 milliseconds. This function will cost twice as much as a Lambda function with 128 MB of memory that runs for the same time.\n\n**Timeout** - You can control the maximum duration of your function by using the timeout configuration. You can set the timeout value for a function to any value up to 15 minutes. When the specified timeout is reached, AWS Lambda stops the running of your Lambda function. Using a timeout can prevent higher costs that come from long-running functions. You must find the right balance between not letting the function run too long and being able to finish under normal circumstances.\n\nFollow these best practices:\n\n- Test the performance of your Lambda function to make sure that you choose the optimum memory size configuration. You can view the memory usage for your function in Amazon CloudWatch Logs.\n- Load-test your Lambda function to analyze how long your function runs and determine the best timeout value. This is important when your Lambda function makes network calls to resources that might not be able to handle the scaling of Lambda functions.\n\nSee the following resources for information about:\n\n- AWS Lambda limits\n- AWS Lambda pricing\n\n#### Demonstration: Creating an AWS Lambda function\n\nNow, the educator might choose to demonstrate how to create an AWS Lambda function.\n\n#### AWS Lambda example: Simulated slot machine browser game\n\nYou can create Lambda functions to perform various tasks. This example uses a browser-based game that simulates a slot machine. The game invokes a Lambda function that generates the random results of each slot pull. The function returns those results as the file names of images that are used to display the result. The images are stored in an Amazon S3 bucket that is configured to function as a static web host for the HTML, CSS, and other assets that are needed to present the application experience.\n\n#### Event-based Lambda function example: Order processing\n\nThis example shows how Lambda can be used in a solution for order processing.\n\nIn this architecture:\n\n1. A customer uploads a transactions file to an S3 bucket, which triggers the running of a Lambda function.\n2. A Lambda function processes the transactions file and updates the Customer and Transactions DynamoDB tables.\n3. Changes to the Transactions DynamoDB table trigger a second Lambda function to aggregate the transactions and update the totals in the Transaction total DynamoDB table. It also pushes a message to the HighBalancerAlert SNS topic.\n4. The HighBalancerAlert SNS topic sends an email notification to the customer, and updates the CreditCollection and CustomerNotify SQS queues for payment processing.\n\n#### Lambda layers\n\n- Enable functions to share code easily – You can upload a layer one time and reference it in any function\n- Promote separation of responsibilities – Developers can iterate faster on writing business logic\n- Enable you to keep your deployment packages small\n- Limits\n    - Up to five layers\n    - 250 MB\n\nWhen you build serverless applications, it is common to have code that is shared across Lambda functions. It can be custom code that two or more functions use, or a standard library that you add to simplify the implementation of your business logic.\n\nPreviously, you packaged and deployed this shared code together with all the functions that used it. Now, you can configure your Lambda function to include additional code and content as layers. A layer is a .zip archive that contains libraries, a custom runtime, or other dependencies.\n\nWith Lambda layers, functions can share code. Developers use layers to upload code one time and reuse it multiple times. With layers, you can use libraries in your function without needing to include them in your deployment package.\n\nSharing code this way can help promote the separation of responsibilities. One person can be responsible for managing the core library. Another person can be responsible for using and building on top of the library code to build application logic.\n\nLayers enable you to keep your deployment package small, which makes development easier.\n\nA function can use up to five layers at a time. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 MB.\n\nFor more information about layers, see AWS Lambda Layers.\n\n#### Demonstration: Using AWS Lambda with Amazon S3\n\nNow, the educator might choose to demonstrate how to configure an Amazon S3 event to trigger a Lambda function.\n\n#### Section 5 key takeaways\n\nSome key takeaways from this section of the module include:\n\n- Lambda is a serverless compute service that provides built-in fault tolerance and automatic scaling.\n- A Lambda function is custom code that you write that processes events.\n- A Lambda function is invoked by a handler, which takes an event object and context object as parameters.\n- An event source is an AWS service or developer-created application that triggers a Lambda function to run.\n- Lambda layers enable functions to share code and keep deployment packages small.\n\n### Module 13 – Guided Lab 2: Implementing a Serverless Architecture on AWS\n\nYou will now complete Module 13 – Guided Lab 2: Implementing a Serverless Architecture on AWS.\n\n#### Guided lab 2: Tasks\n\n1. Create a Lambda function to load data\n2. Configure an Amazon S3 event\n3. Test the loading process\n4. Configure notifications\n5. Create a Lambda function to send notifications\n6. Test the system\n\n#### Guided lab 2: Final product\n\nThe diagram summarizes what you will have built after you complete the lab.\n\n#### Begin Module 13 – Guided Lab 2: Implementing a Serverless Architecture on AWS\n\nIt is now time to start the guided lab.\n\n#### Guided lab 2 debrief: Key takeaways\n\nYour educator might choose to lead a conversation about the key takeaways from this guided lab after you have completed it.\n\n### Section 6: Extending serverless architectures with Amazon API Gateway\n\n#### Introducing Section 6: Extending serverless architectures with Amazon API Gateway.\n\n#### Amazon API Gateway\n\n- Enables you to create, publish, maintain, monitor, and secure APIs that act as entry points to backend resources for your applications\n- Handles up to hundreds of thousands of concurrent API calls\n- Can handle workloads that run on\n    - Amazon EC2\n    - Lambda\n    - Any web application\n    - Real-time communication applications\n- Can host and use multiple versions and stages of your APIs\n\nAmazon API Gateway is a fully managed service that enables you to create, publish, maintain, monitor, and secure APIs at any scale. You can use it to create Representational State Transfer (RESTful) and WebSocket APIs that act as an entry point for applications so they can access backend resources. Applications can then access data, business logic, or functionality from your backend services. Such services include applications that run on Amazon EC2, code that runs on Lambda, any web application, or real-time communication applications.\n\nAPI Gateway handles all the tasks that are involved in accepting and processing up to hundreds of thousands of concurrent API calls. Such calls might include traffic management, authorization and access control, monitoring, and API version management. API Gateway has no minimum fees or startup costs. You pay only for the API calls you receive and the amount of data that is transferred out. With the API Gateway tiered-pricing model, you can reduce your cost as your API usage scales.\n\nYou can use API Gateway to host multiple versions and stages of your APIs.\n\n#### Amazon API Gateway security\n\nWhen you make your APIs publicly available, you are exposed to attackers that try to exploit your services. With Amazon API Gateway, you can protect your APIs in several ways.\n\nWith Amazon API Gateway, you can optionally set your API methods to require authorization. When you set up a method to require authorization, you can use AWS Signature Version 4 or Lambda authorizers to support your own bearer token authentication strategy. AWS Signature Version 4 is the process to add authentication information to AWS requests sent through HTTP. For security, most requests to AWS must be signed with an access key, which consists of an access key ID and secret access key. You use these AWS credentials to sign requests to your service and authorize access, like other AWS services. You can retrieve temporary credentials that are associated with a role in your AWS account by using Amazon Cognito. A Lambda authorizer is a Lambda function that authorizes access to APIs by using a bearer token authentication strategy like OAuth.\n\nYou can also apply a resource policy to an API to restrict access to a specific Amazon VPC or VPC endpoint. You can give an Amazon VPC or VPC endpoint from a different account access to the private API by using a resource policy.\n\nAmazon API Gateway supports throttling settings for each method or route in your APIs. You can set a standard rate limit and a burst rate limit per second for each method in your REST APIs and each route in WebSocket APIs.\n\nAdditionally, you can use AWS WAF to secure your API Gateway APIs. AWS WAF is a web application firewall that helps protect your web applications from common web exploits that could affect availability, compromise security, or consume excessive resources.\n\nFor more information about how to secure your APIs with Amazon API Gateway, see the Security & Authorization section of Amazon API Gateway FAQs.\n\n#### Amazon API Gateway: Common architecture example\n\nYou can use Amazon API Gateway to provide the API layer for your applications. Here is an example of a common architecture with Amazon API Gateway.\n\nIn this example, front-end client applications and application services send traffic to API Gateway over the internet. Often, Amazon CloudFront is used to cache static content. API Gateway abstracts and exposes APIs that can call various backend applications. These applications include Lambda functions, Docker containers running on EC2 instances, virtual private clouds (VPCs), or any publicly accessible endpoint. If necessary, API Gateway can cache the response. Finally, all the API calls can be monitored with Amazon CloudWatch.\n\nYou can use API Gateway with other AWS managed services to build serverless backends for your applications. For example, API Gateway can proxy requests to Lambda functions that run your code and generate responses. You can even create APIs that proxy requests to other AWS services, such as Amazon Simple Storage Service (Amazon S3), without having to write any code. You can get production-scale backends running more quickly because you have less code to write, and you also offloaded operational burden to the AWS services.\n\nFor an example of how to use API Gateway with AWS Lambda, see this AWS blog post.\n\n#### Example: RESTful microservices\n\nAmazon API Gateway is deeply integrated with AWS Lambda. This example shows how the two services can be used together in an architecture for a RESTful microservice application.\n\nIn this example,", "# AWS Academy Cloud Architecting\n\n## Module 13: Building Microservices and Serverless Architectures\n\n### 1. Contents\n\n- **Module 13: Building Microservices and Serverless Architectures** 4\n\n### 2. Module Overview\n\n#### 2.1. Sections\n\n1. Architectural Need\n2. Introducing Microservices\n3. Building Microservice Applications with AWS Container Services\n4. Introducing Serverless Architectures\n5. Building Serverless Architectures with AWS Lambda\n6. Extending Serverless Architectures with Amazon API Gateway\n7. Orchestrating Microservices with AWS Step Functions\n\n#### 2.2. Demonstrations\n\n- Creating an AWS Lambda function\n- Using AWS Lambda with Amazon S3\n\n#### 2.3. Labs\n\n- **(Optional)** Guided Lab 1: Breaking a Monolithic Node.js Application into Microservices\n- Guided Lab 2: Implementing a Serverless Architecture on AWS\n- Challenge Lab: Implementing a Serverless Architecture for the Café\n\n#### 2.4. Knowledge Check\n\nThis module covers the following topics:\n\n- Architectural need\n- Introducing microservices\n- Building microservice applications with AWS container services\n- Introducing serverless architectures\n- Building serverless architectures with AWS Lambda\n- Extending serverless architectures with Amazon API Gateway\n- Orchestrating microservices with AWS Step Functions\n\n**In addition to the above sections, this module includes:**\n\n- Two AWS Lambda demonstrations\n- An optional guided lab where you refactor a monolithic application into microservices\n- A guided lab where you implement a serverless architecture on AWS with Amazon S3, AWS Lambda, Amazon DynamoDB, and Amazon SNS\n- A challenge lab where you use AWS Lambda and Amazon Simple Notification Service (Amazon SNS) to generate and send a daily sales report for the café.\n\nFinally, you will be asked to complete a knowledge check that will test your understanding of key concepts covered in this module.\n\n### 3. Module Objectives\n\nAt the end of this module, you should be able to:\n\n- Indicate the characteristics of microservices\n- Refactor a monolithic application into microservices and use Amazon ECS to deploy the containerized microservices\n- Explain serverless architecture\n- Implement a serverless architecture with AWS Lambda\n- Describe a common architecture for Amazon API Gateway\n- Describe the types of workflows that AWS Step Functions supports\n\n### 4. Section 1: Architectural Need\n\n#### 4.1. Introducing Section 1: Architectural Need\n\n#### 4.2. Café Business Requirement\n\nThe café wants to get daily reports via email about all the orders that were placed on the website. They want this information so they can anticipate demand and bake the correct number of desserts going forward (reducing waste). They also want to identify any patterns in their business (analytics).\n\nFrank and Martha want to get daily reports via email about all the orders that were placed on the website. Frank wants to anticipate demand so he can bake the correct number of desserts going forward (reducing waste). Martha wants to identify any patterns in the café's business (analytics). Currently, Sofía has set up a cron job on the web server instance that sends these daily order report email messages to Frank and Martha. However, the cron job is resource-intensive and reduces web server performance.\n\nOlivia advises Sofía and Nikhil that non-business-critical reporting tasks should be kept separate. Sofía and Nikhil want to further decouple the architecture and move the cron job into a managed, serverless environment that will scale well and reduce costs.\n\n### 5. Section 2: Introducing Microservices\n\n#### 5.1. Introducing Section 2: Introducing Microservices\n\n#### 5.2. What are Microservices?\n\nApplications that are composed of independent services that communicate over well-defined APIs.\n\nMicroservices are an architectural and organizational approach to software development where applications are composed of independent services that communicate over well-defined application programming interfaces (APIs). This approach is designed to speed up deployment cycles.\n\nThe microservices approach fosters innovation and ownership, and improves the maintainability and scalability of software applications.\n\n#### 5.3. Monolithic versus Microservice Applications\n\nTo understand the benefits of microservices, consider first a monolithic application.\n\nIn the example on the left, the three processes (users, topics, and messages) of a monolithic forum application are tightly coupled. They run as a single service. If one process of the application experiences a spike in demand, the entire architecture must be scaled. Adding or improving features becomes more complex as the code base grows, which limits experimentation and makes it difficult to implement new ideas. The availability of monolithic applications is also at risk because many dependent and tightly coupled processes increase the impact of a single process failure.\n\nNow, suppose that the same application runs in a microservice architecture. Each process of the application is built as an independent component that runs as a service. The services communicate by using lightweight API operations. Each service performs a single function that can support multiple applications. Because the services run independently, they can be updated, deployed, and scaled to meet the demand for specific functions of an application.\n\nA microservice architecture provides much quicker iteration, automation, and overall agility. Start fast, fail fast, and recover fast.\n\nFor an overview of microservices on AWS, see What are Microservices?\n\n#### 5.4. Characteristics of Microservices\n\nMicroservices share some common characteristics:\n\n- **Decentralized** – Microservice architectures are distributed systems with decentralized data management. They don't rely on a unifying schema in a central database. Each microservice has its own view about data models. Microservices are also decentralized in the way they are developed, deployed, managed, and operated.\n- **Independent** – Each component service in a microservice architecture can be changed, upgraded, or replaced independently without affecting the function of other services. Services do not need to share any of their code or implementation with other services. Similarly, the teams responsible for different microservices can act independently from each other.\n- **Specialized** - Each component service is designed for a set of capabilities and focuses on a specific domain. If the code for a particular component service reaches a certain level of complexity, then the service can be split into two or more services.\n- **Polyglot** - Microservices don't follow a single approach. Teams have the freedom to choose the best tool for their specific problem. As a consequence, microservice architectures take a heterogeneous approach to operating systems, programming languages, data stores, and tools. This approach is called polyglot persistence and programming.\n- **Black boxes** - Individual component services are designed as black boxes, which mean that the details of their complexity are hidden from other components. Any communication between services happens through well-defined APIs to prevent implicit and hidden dependencies.\n- **You build it, you run it** – DevOps is a key organizational principle for microservices, where the team responsible for building a service is also responsible for operating and maintaining it in production.\n\n#### 5.5. Section 2 Key Takeaways\n\nSome key takeaways from this section of the module include:\n\n- Microservice applications are composed of independent services that communicate over well-defined APIS\n- Microservices share the following characteristics:\n    - Decentralized: Microservices are decentralized in the way they are developed, deployed, managed, and operated\n    - Independent: Each component service in a microservices architecture can be developed, deployed, operated, and scaled without affecting the function of other services\n    - Specialized: Each component service is designed for a set of capabilities and focuses on solving a specific problem\n    - Polyglot: Microservice architectures take a heterogeneous approach to operating systems, programming languages, data stores, and tools\n    - Black boxes: The details of the complexity of microservice components are hidden from other components\n    - You build it, you run it: DevOps is a key organizational principle for microservices\n\n### 6. Section 3: Building Microservice Applications with AWS Container Services\n\n#### 6.1. Introducing Section 3: Building Microservice Applications with AWS Container Services\n\n#### 6.2. What is a Container?\n\nWhen you build a microservice architecture, you can use containers for the processing power.\n\nContainers are a method of operating system virtualization that enables you to run an application and its dependencies in resource-isolated processes. A container is a lightweight, standalone software package. It contains everything that a software application needs to run, such as the application code, runtime engine, system tools, system libraries, and configurations.\n\n#### 6.3. A Problem that Containers Solve\n\nContainers can help ensure that applications deploy quickly, reliably, and consistently, regardless of deployment environment. Containers also give you more granular control over resources, which improves the efficiency of your infrastructure.\n\n#### 6.4. Container Terminology\n\nA container is created from a read-only template that is called an image. Images are typically built from a Dockerfile, which is a plaintext file that specifies all the components that are included in the container. You can create images from scratch, or you can use images that others created and published to a public or private container registry.\n\nA container image is the snapshot of the file system that is available to the container. For example, you might have the Debian operating system as a container image. When you run this container, a Debian operating system is available to it. You can also package all your code dependencies in the container image and use it as your code artifact.\n\nContainer images are stored in a registry. You can download the images from the registry and run them on your cluster. Registries can exist in or outside your AWS infrastructure.\n\n#### 6.5. Amazon ECS\n\nYou can run your containers on Amazon Elastic Container Service (Amazon ECS). Amazon ECS is a highly scalable, high-performance, container-management service. It supports Docker containers and enables you to easily run applications on a managed cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances.\n\nAmazon ECS is a scalable cluster service for hosting containers that:\n\n- Can scale up to thousands of Docker containers in seconds\n- Monitors container deployment\n- Manages the state of the cluster that runs the containers\n- Schedules containers by using a built-in scheduler or third-party scheduler (Apache Mesos, Blox)\n- Is extensible by using APIs\n- Can be launched with either AWS Fargate or Amazon EC2 launch types\n\nYou can run ECS clusters at scale by mixing Spot Instances with On-Demand Instances and Reserved Instances.\n\n#### 6.6. Amazon ECS Orchestrates Containers\n\nAmazon ECS is a regional service that simplifies running application containers in a highly available manner across multiple Availability Zones within a Region. You can create ECS clusters in a new or existing virtual private cloud (VPC). A cluster is a logical grouping of resources.\n\nAfter a cluster is up and running, you can define task definitions and services that specify which Docker container images to run across your clusters.\n\nA task definition is a text file in JavaScript Object Notation (JSON) format. It describes one or more containers, up to a maximum of 10, that form your application. You can think of it as a blueprint for your application. Task definitions specify parameters for your application—for example, which containers and launch type to use. Other parameters include which ports should be opened for your application and what data volumes should be used with the containers in the task.\n\nA service enables you to specify how many copies of your task definition to run and maintain in a cluster. You can optionally use an Elastic Load Balancing load balancer to distribute incoming traffic to containers in your service. Amazon ECS maintains that number of tasks and coordinates task scheduling with the load balancer.\n\nAfter you create a task definition for your application, you can specify the number of tasks that will run on your cluster. A task is the instantiation of a task definition within a cluster. When you use Amazon ECS to run tasks, you place them in a cluster. Amazon ECS downloads your container images from a registry that you specify, and runs those images within your cluster.\n\n#### 6.7. Amazon ECS Launch Types\n\nAmazon ECS offers two launch types for hosting your containerized applications.\n\nYou can use the Fargate launch type to host your cluster on a serverless infrastructure that Amazon ECS manages. You only need to package your application in containers, specify the CPU and memory requirements, define networking and AWS Identity and Access Management (IAM) policies, and launch the application.\n\nAlternatively, if you want more control, you can use the EC2 launch type to host your tasks on a cluster of EC2 container instances that you manage. A container instance is an EC2 instance that is running the Amazon ECS container agent. You can use Amazon ECS to schedule the placement of containers across your cluster based on your resource needs, isolation policies, and availability requirements. For information about different scheduling options, see Scheduling Amazon ECS Tasks. Amazon ECS keeps track of all the CPU, memory, and other resources in your cluster. It also finds the best server for a container to run on based on your specified resource requirements.\n\nFor more information about the Fargate and EC2 launch types, see Amazon ECS Launch Types.\n\n#### 6.8. Amazon ECS Cluster Auto Scaling\n\nYou can create an Auto Scaling group for an Amazon ECS cluster. The Auto Scaling group contains container instances that you can scale out (and in) by using Amazon CloudWatch alarms. If you configure your Auto Scaling group to remove container instances, any tasks that are running on the removed container instances are stopped. If your tasks are running as part of a service, Amazon ECS restarts those tasks on another instance if the required resources are available. Examples of such required resources include CPU, memory, ports. However, tasks that were started manually are not restarted automatically.\n\nYou can also take advantage of Amazon ECS cluster auto scaling, which gives you more control over how you scale tasks in a cluster. It increases the speed and reliability of cluster scale-out. It gives you control over the amount of spare capacity that is maintained in your cluster, and automatically manages instance termination on scale-in.\n\nWith cluster auto scaling, you can configure Amazon ECS to scale your Auto Scaling group in and out automatically. Cluster auto scaling relies on capacity providers, which link your ECS cluster to the Auto Scaling groups that you want to use. Each Auto Scaling group is associated with a capacity provider, and each capacity provider has only one Auto Scaling group. However, many capacity providers can be associated with one ECS cluster. To scale the entire cluster automatically, each capacity provider manages the scaling of its associated Auto Scaling group.\n\nFor more information about cluster auto scaling, see the Amazon ECS Cluster Auto Scaling AWS News Blog post.\n\n#### 6.9. Decomposing Monoliths – Step 1: Create Container Images\n\nAgain, consider the monolithic forum application that you saw earlier where the entire application runs as a single service. To rearchitect this application by using a microservice architecture, you can run each application process as a separate service within its own container. With a microservice architecture, the services can scale and be updated independently of the others.\n\nTo deploy the monolithic application as a microservice application, first build and tag an image for each service. Then, register the images with Amazon Elastic Container Registry (Amazon ECR).\n\n#### 6.10. Decomposing Monoliths – Step 2: Create Service Task Definition and Target Groups\n\nNext, choose a launch type and create a new service for each piece of the original monolithic application. Amazon ECS deploys each service into its own container across an ECS cluster. Then, create a target group for each service. The target group tracks the instances and ports of each container that is running for that service.\n\n#### 6.11. Decomposing Monoliths – Step 3: Connect Load Balancer to Services\n\nFinally, create an Application Load Balancer and configure listener rules to connect to the services. The listener checks for incoming connection requests to your load balancer and uses the rules to route traffic appropriately. In the example, the listener for the Application Load Balancer listens for HTTP service requests on Port 80 and routes them to the appropriate service.\n\n#### 6.12. Tools for Building Highly Available Microservice Architectures\n\nAWS Cloud Map and AWS App Mesh are two tools that can help you build highly available microservice architectures.\n\n**AWS Cloud Map**\n\n- Is a fully managed discovery service for cloud resources\n- Can be used to define custom names for application resources\n- Maintains updated location of dynamically changing resources, which increases application availability\n\n**AWS App Mesh**\n\n- Captures metrics, logs, and traces from all your microservices\n- Enables you to export this data to Amazon CloudWatch, AWS X-Ray, and compatible AWS Partner Network (APN) Partner and community tools\n- Enables you to control traffic flows between microservices to help ensure that services are highly available\n\nAWS Cloud Map is a fully managed discovery service for cloud resources. You can use it to define custom names for your application resources (such as databases, queues, microservices, and other cloud resources). AWS Cloud Map maintains the updated location of these dynamically changing resources. This location maintenance increases the availability of your application because your web service always discovers the most up-to-date locations of its resources. You can add and register any resource with minimal manual intervention of mappings. AWS Cloud Map assists with service discovery, continuous integration, and health monitoring of your microservices and applications.\n\nFor more information about AWS Cloud Map, read this AWS Open Source Blog post. To learn more about how you can use AWS Cloud Map to enable your containerized services to discover and connect with each other, read AWS Fargate, Amazon EKS, and Amazon ECS now integrate with AWS Cloud Map.\n\nWhen you create your task definitions, you can enable App Mesh integration. AWS App Mesh captures metrics, logs, and traces from all of your microservices. You can export this data to Amazon CloudWatch, AWS X-Ray, and compatible AWS Partner Network (APN) Partner and community tools for monitoring and tracing. AWS App Mesh also enables you to control how traffic flows between your microservices to make sure that every service is highly available during deployments, after failures, and as your application scales.\n\nApp Mesh enables you to configure microservices to connect directly to each other via a proxy instead of requiring code within the application or by using a load balancer. App Mesh uses Envoy, an open source service-mesh proxy, which is deployed alongside your microservice containers.\n\nFor more information about AWS Cloud Map and AWS App Mesh, see this AWS YouTube video.\n\n#### 6.13. AWS Fargate\n\n- Is a fully managed container service\n- Works with Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Kubernetes Service (Amazon EKS)\n- Provisions, manages, and scales your container clusters\n- Manages runtime environment\n- Provides automatic scaling\n\nIn this section, you have learned that Amazon ECS offers two launch types: EC2 and Fargate.\n\nAWS Fargate is a fully managed container service that works with both Amazon ECS and Amazon Elastic Kubernetes Service (Amazon EKS). It enables you to run containers without needing to manage servers or clusters. With AWS Fargate, you no longer need to provision, configure, and scale clusters of virtual machines to run containers. As a result, you don't need to choose server types, decide when to scale your clusters, or optimize cluster packing. AWS Fargate reduces the need for you to interact with or think about servers or clusters. Fargate enables you to focus on designing and building your applications instead of managing the infrastructure that runs them.\n\n#### 6.14. Section 3 Key Takeaways\n\nSome key takeaways from this section of the module include:\n\n- Amazon ECS is a highly scalable, high-performance container management service. It supports Docker containers and enables you to easily run applications on a managed cluster of Amazon EC2 instances.\n- Cluster auto scaling gives you more control over how you scale tasks within a cluster.\n- AWS Cloud Map enables you to define custom names for your application resources. It maintains the updated location of these dynamically changing resources.\n- AWS App Mesh is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure.\n- AWS Fargate is a fully managed container service that enables you to run containers without needing to manage servers or clusters.\n\n#### 6.15. Module 13 – Guided Lab 1: Breaking a Monolithic Node.js Application into Microservices \n\n(Optional lab)\n\nYou might choose to complete Module 13 – Guided Lab 1: Breaking a Monolithic Node.js Application into Microservices. This lab is optional.\n\n#### 6.16. Guided Lab 1: Tasks\n\n1. Prepare the AWS Cloud9 development environment\n2. Run a monolithic application on a basic Node.js server\n3. Containerize the monolith for Amazon ECS\n4. Deploy the monolith to Amazon ECS\n5. Refactor the monolith into containerized microservices\n\n#### 6.17. Guided Lab 1: Final Product\n\nThe diagram summarizes what you will have built after you complete the lab.\n\n#### 6.18. Begin Module 13 – Guided Lab 1: Breaking a Monolithic Node.js Application into Microservices\n\nIt is now time to start the optional guided lab.\n\n#### 6.19. Guided Lab 1 Debrief: Key Takeaways\n\nYour educator might choose to lead a conversation about the key takeaways from this guided lab after you have completed it.\n\n### 7. Section 4: Introducing Serverless Architectures\n\n#### 7.1. Introducing Section 4: Introducing Serverless Architectures\n\n#### 7.2. What Does Serverless Mean?\n\nA way for you to build and run applications and services without thinking about servers.\n\nSo far, you have learned that you can use Amazon ECS to build your microservice applications by using containers. Amazon ECS is a container orchestration service where you manage your application code, data source integrations, security configuration, updates, network configuration, firewall, and management tasks. You also learned that you can use the Fargate launch type to host your cluster on a serverless infrastructure that Amazon ECS manages.\n\nBut what does serverless mean?\n\nServerless is the native architecture of the cloud that enables you to shift more operational responsibilities to AWS, which can increase your agility and innovation. Serverless enables you to build and run applications and services without thinking about servers. Your application still runs on servers. However, AWS does all the server management tasks, such as server or cluster provisioning, patching, operating system maintenance, and capacity provisioning.\n\n#### 7.3. Tenets of Serverless Architectures\n\nThe tenets that define serverless as an operational model include:\n\n- No infrastructure to provision or manage (no servers to provision, operate, or patch)\n- Automatically scales by unit of consumption (scales by unit of work or consumption rather than by server unit)\n- Pay-for-value pricing model (you pay only for the duration that a resource runs, rather than by server unit)\n- Built-in availability and fault tolerance (no need to architect for availability because it is built into the service)\n\nFor more information about what serverless is, see this AWS website.\n\n#### 7.4. Benefits of Serverless\n\nServerless enables you to build modern applications with increased agility and lower total cost of ownership (TCO). By using a serverless architecture, you can focus on your core product. You don't need to worry about managing and operating servers or runtimes, either in the cloud or on premises. This reduced overhead enables you to reclaim time and energy, which you can spend on developing products that scale and are reliable. Finally, serverless architectures enable you to build microservice applications.\n\n#### 7.5. AWS Serverless Offerings\n\nAWS has many offerings that you can use to build serverless architectures on AWS. So far in this course, you have already learned about several of them.\n\nThe rest of this module focuses on how you can use AWS Lambda, Amazon API Gateway, and AWS Step Functions to build serverless architectures.\n\n#### 7.6. Section 4 Key Takeaways\n\nSome key takeaways from this section of the module include:\n\n- Serverless computing enables you to build and run applications and services without provisioning or managing servers\n- Serverless architectures offer the following benefits:\n    - Lower TCO\n    - You can focus on your application\n    - You can use them to build microservice applications\n\n### 8. Section 5: Building Serverless Architectures with AWS Lambda\n\n#### 8.1. Introducing Section 5: Building Serverless Architectures with AWS Lambda\n\n#### 8.2. AWS Lambda\n\n- Is a fully managed compute service\n- Runs your code on a schedule or in response to events (for example, changes to an Amazon S3 bucket or an Amazon DynamoDB table)\n- Supports Java, Go, PowerShell, Node.js, C#, Python, Ruby, and Runtime API\n- Can run at edge locations closer to your users\n\nAWS Lambda is a fully managed compute service that runs your code in response to events and automatically manages the underlying compute resources for you. Lambda runs your code on a high-availability compute infrastructure and performs all administration of the compute resources, including server and operating system maintenance, capacity provisioning, automatic scaling, code monitoring, and logging.\n\nAWS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code, and provides a Runtime API that enables you to use any additional programming languages to author your functions.\n\nLambda@Edge is a feature of Amazon CloudFront that enables you to run code closer to users of your application, which improves performance and reduces latency. Lambda@Edge runs your code in response to events that are generated by the Amazon CloudFront content delivery network (CDN). Lambda@Edge enables you to run Node.js and Python Lambda functions to customize content that Amazon CloudFront delivers. For information about how to add HTTP security response headers, read this AWS Networking & Content Delivery Blog post.\n\n#### 8.3. How AWS Lambda Works\n\nAWS Lambda integrates with other AWS services to invoke Lambda functions. A Lambda function is custom code that you write in one of the languages that Lambda supports. You can configure triggers to invoke a function in response to resource lifecycle events, respond to incoming HTTP requests, consume events from a queue, or run on a schedule.\n\nAn event source is the entity that publishes the event to Lambda. Your Lambda function processes the event, and Lambda runs your Lambda function on your behalf.\n\nLambda functions are stateless, which means that they have no affinity to the underlying infrastructure. Lambda can rapidly launch as many copies of the function as needed to scale to the rate of incoming events.\n\n#### 8.4. Lambda Functions\n\nWhen you create a Lambda function, you define the permissions for the function and specify which events trigger the function. You also create a deployment package that includes your application code and any dependencies and libraries that are needed to run your code. Finally, you configure runtime parameters such as memory, time out, and concurrency. When your function is invoked, Lambda will run an environment based on the runtime and configuration options that you selected.\n\n#### 8.5. Anatomy of a Lambda Function\n\nWhen a Lambda function is invoked, the code begins running at the handler. The handler is a specific code method or function that you create and include in your package. You specify the handler when you create a Lambda function. Each supported language has its own requirements for how a function handler can be defined and referenced within the package. After the handler is successfully invoked inside your Lambda function, the runtime environment belongs to the code you wrote.\n\nThe handler always takes two objects: the event object and the context object.\n\nThe event object provides information about the event that triggered the Lambda function. This event might be a pre-defined object that an AWS service generates, or a custom user-defined object in the form of a serializable string. An example of such a string might be a plain old Java object (POJO) or a JSON stream.\n\nThe contents of the event object include all the data and metadata that your Lambda function needs to drive its logic. The contents and structure of the event object vary, depending on which event source created it. For example, an event that is created by API Gateway contains details that are related to the HTTPS request that was made by the API client—such as path, query string, and request body. However, an event that is created by Amazon includes details about the bucket and the new object.\n\nThe context object is generated by AWS and provides metadata about the runtime environment. The context object enables your function code to interact with the Lambda runtime environment. The contents and structure of the context object vary based on the language runtime that your Lambda function uses.\n\nHowever, at a minimum, the context object contains:\n\n- awsRequestId – This property is used to track specific invocations of a Lambda function (important for error reporting or when contacting AWS Support)\n- logStreamName – The CloudWatch log stream that your log statements will be sent to\n- getRemainingTimeInMillis() – This method returns the number of milliseconds that remain before the running of your function times out\n\n#### 8.6. Lambda Function Configuration and Billing\n\nMemory and timeout are configurations that determine how your Lambda function performs. These configurations affect your billing. With AWS Lambda, you are charged based on the number of requests for your functions (the total number of requests across all your functions) and the duration (the time it takes for your code to run). The price depends on the amount of memory you allocate to your function.\n\n**Memory** - You specify the amount of memory you want to allocate to your Lambda function. Lambda then allocates CPU power that is proportional to the memory. Lambda is priced so that the cost per 1 ms of function duration increases as the memory configuration increases. For example, say that you have a Lambda function with 256 MB of memory, and that it runs for 110 milliseconds. This function will cost twice as much as a Lambda function with 128 MB of memory that runs for the same time.\n\n**Timeout** - You can control the maximum duration of your function by using the timeout configuration. You can set the timeout value for a function to any value up to 15 minutes. When the specified timeout is reached, AWS Lambda stops the running of your Lambda function. Using a timeout can prevent higher costs that come from long-running functions. You must find the right balance between not letting the function run too long and being able to finish under normal circumstances.\n\nFollow these best practices:\n\n- Test the performance of your Lambda function to make sure that you choose the optimum memory size configuration. You can view the memory usage for your function in Amazon CloudWatch Logs.\n- Load-test your Lambda function to analyze how long your function runs and determine the best timeout value. This is important when your Lambda function makes network calls to resources that might not be able to handle the scaling of Lambda functions.\n\nSee the following resources for information about:\n\n- AWS Lambda limits\n- AWS Lambda pricing\n\n#### 8.7. Demonstration: Creating an AWS Lambda Function\n\nNow, the educator might choose to demonstrate how to create an AWS Lambda function.\n\n#### 8.8. AWS Lambda Example: Simulated Slot Machine Browser Game\n\nYou can create Lambda functions to perform various tasks. This example uses a browser-based game that simulates a slot machine. The game invokes a Lambda function that generates the random results of each slot pull. The function returns those results as the file names of images that are used to display the result. The images are stored in an Amazon S3 bucket that is configured to function as a static web host for the HTML, CSS, and other assets that are needed to present the application experience.\n\n#### 8.9. Event-Based Lambda Function Example: Order Processing\n\nThis example shows how Lambda can be used in a solution for order processing.\n\nIn this architecture:\n\n1. A customer uploads a transactions file to an S3 bucket, which triggers the running of a Lambda function.\n2. A Lambda function processes the transactions file and updates the Customer and Transactions DynamoDB tables.\n3. Changes to the Transactions DynamoDB table trigger a second Lambda function to aggregate the transactions and update the totals in the Transaction total DynamoDB table. It also pushes a message to the HighBalancerAlert SNS topic.\n4. The HighBalancerAlert SNS topic sends an email notification to the customer, and updates the CreditCollection and CustomerNotify SQS queues for payment processing.\n\n#### 8.10. Lambda Layers\n\n- Enable functions to share code easily – You can upload a layer one time and reference it in any function\n- Promote separation of responsibilities – Developers can iterate faster on writing business logic\n- Enable you to keep your deployment packages small\n- Limits:\n    - Up to five layers\n    - 250 MB\n\nWhen you build serverless applications, it is common to have code that is shared across Lambda functions. It can be custom code that two or more functions use, or a standard library that you add to simplify the implementation of your business logic.\n\nPreviously, you packaged and deployed this shared code together with all the functions that used it. Now, you can configure your Lambda function to include additional code and content as layers. A layer is a .zip archive that contains libraries, a custom runtime, or other dependencies.\n\nWith Lambda layers, functions can share code. Developers use layers to upload code one time and reuse it multiple times. With layers, you can use libraries in your function without needing to include them in your deployment package.\n\nSharing code this way can help promote the separation of responsibilities. One person can be responsible for managing the core library. Another person can be responsible for using and building on top of the library code to build application logic.\n\nLayers enable you to keep your deployment package small, which makes development easier.\n\nA function can use up to five layers at a time. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 MB.\n\nFor more information about layers, see AWS Lambda Layers.\n\n#### 8.11. Demonstration: Using AWS Lambda with Amazon S3\n\nNow, the educator might choose to demonstrate how to configure an Amazon S3 event to trigger a Lambda function.\n\n#### 8.12. Section 5 Key Takeaways\n\nSome key takeaways from this section of the module include:\n\n- Lambda is a serverless compute service that provides built-in fault tolerance and automatic scaling.\n- A Lambda function is custom code that you write that processes events.\n- A Lambda function is invoked by a handler, which takes an event object and context object as parameters.\n- An event source is an AWS service or developer-created application that triggers a Lambda function to run.\n- Lambda layers enable functions to share code and keep deployment packages small.\n\n### 9. Module 13 – Guided Lab 2: Implementing a Serverless Architecture on AWS\n\nYou will now complete Module 13 – Guided Lab 2: Implementing a Serverless Architecture on AWS.\n\n#### 9.1. Guided Lab 2: Tasks\n\n1. Create a Lambda function to load data\n2. Configure an Amazon S3 event\n3. Test the loading process\n4. Configure notifications\n5. Create a Lambda function to send notifications\n6. Test the system\n\n#### 9.2. Guided Lab 2: Final Product\n\nThe diagram summarizes what you will have built after you complete the lab.\n\n#### 9.3. Begin Module 13 – Guided Lab 2: Implementing a Serverless Architecture on AWS\n\nIt is now time to start the guided lab.\n\n#### 9.4. Guided Lab 2 Debrief: Key Takeaways\n\nYour educator might choose to lead a conversation about the key takeaways from this guided lab after you have completed it.\n\n### 10. Section 6: Extending Serverless Architectures with Amazon API Gateway\n\n#### 10.1. Introducing Section 6: Extending Serverless Architectures with Amazon API Gateway\n\n#### 10.2. Amazon API Gateway\n\n- Enables you to create, publish, maintain, monitor, and secure APIs that act as entry points to backend resources for your applications\n- Handles up to hundreds of thousands of concurrent API calls\n- Can handle workloads that run on:\n    - Amazon EC2\n    - Lambda\n    - Any web application\n    - Real-time communication applications\n- Can host and use multiple versions and stages of your APIs\n\nAmazon API Gateway is a fully managed service that enables you to create, publish, maintain, monitor, and secure APIs at any scale. You can use it to create Representational State Transfer (RESTful) and WebSocket APIs that act as an entry point for applications so they can access backend resources. Applications can then access data, business logic, or functionality from your backend services. Such services include applications that run on Amazon EC2, code that runs on Lambda, any web application, or real-time communication applications.\n\nAPI Gateway handles all the tasks that are involved in accepting and processing up to hundreds of thousands of concurrent API calls. Such calls might include traffic management, authorization and access control, monitoring, and API version management. API Gateway has no minimum fees or startup costs. You pay only for the API calls you receive and the amount of data that is transferred out. With the API Gateway tiered-pricing model, you can reduce your cost as your API usage scales.\n\nYou can use API Gateway to host multiple versions and stages of your APIs.\n\n#### 10.3. Amazon API Gateway Security\n\nWhen you make your APIs publicly available, you are exposed to attackers that try to exploit your services. With Amazon API Gateway, you can protect your APIs in several ways.\n\nWith Amazon API Gateway, you can optionally set your API methods to require authorization. When you set up a method to require authorization, you can use AWS Signature Version 4 or Lambda authorizers to support your own bearer token authentication strategy. AWS Signature Version 4 is the process to add authentication information to AWS requests sent through HTTP. For security, most requests to AWS must be signed with an access key, which consists of an access key ID and secret access key. You use these AWS credentials to sign requests to your service and authorize access, like other AWS services. You can retrieve temporary credentials that are associated with a role in your AWS account by using Amazon Cognito. A Lambda authorizer is a Lambda function that authorizes access to APIs by using a bearer token authentication strategy like OAuth.\n\nYou can also apply a resource policy to an API to restrict access to a specific Amazon VPC or VPC endpoint. You can give an Amazon VPC or VPC endpoint from a different account access to the private API by using a resource policy.\n\nAmazon API Gateway supports throttling settings for each method or route in your APIs. You can set a standard rate limit and a burst rate limit per second for each method in your REST APIs and each route in WebSocket APIs.\n\nAdditionally, you can use AWS WAF to secure your API Gateway APIs. AWS WAF is a web application firewall that helps protect your web applications from common web exploits that could affect availability, compromise security, or consume excessive resources"]