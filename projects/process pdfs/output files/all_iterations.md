2024-09-08 01:38:41.925738;

['## 3.1 Contents\n\n- **3.2 Design Principles and Patterns for Data Pipelines** \n\n## 3.2 Design Principles and Patterns for Data Pipelines\n\n### 3.2.1 Introduction\n\nThis module focuses on designing data pipelines using the AWS Well-Architected Framework. It delves into the evolution of data stores and architectures, showcasing the transition from traditional systems to modern, cloud-based solutions.  You will learn about the components of modern data architectures on AWS, including key services for building streaming analytics pipelines. \n\n### 3.2.2 Module Objectives\n\nThis module aims to equip you with the knowledge to:\n\n- **Utilize the AWS Well-Architected Framework** to guide the design of analytics workloads.\n- **Trace the historical progression** of data stores and data architectures.\n- **Describe the key components** of modern data architectures on AWS.\n- **Identify AWS design considerations and essential services** for a streaming analytics pipeline.\n\nThis module builds upon previous modules that introduced data pipelines and fundamental data characteristics. It reinforces the AWS Well-Architected Framework, a valuable resource for designing new pipelines and optimizing existing infrastructure. By understanding the evolution of data stores, you will gain a deeper appreciation for the modern data architecture, which provides a comprehensive framework for designing solutions that modernize, unify, and innovate. This module also explores the characteristics of streaming data pipelines, setting the stage for understanding various concepts and use cases covered in later modules.\n\n### 3.2.3 Module Overview\n\n#### 3.2.3.1 Presentation Sections:\n\n- **AWS Well-Architected Framework and Lenses**\n- **The Evolution of Data Architectures**\n- **Modern Data Architecture on AWS**\n- **Modern Data Architecture Pipeline: Ingestion and Storage**\n- **Modern Data Architecture Pipeline: Processing and Consumption**\n- **Streaming Analytics Pipeline**\n\n#### 3.2.3.2 Activity:\n\n- **Using the Well-Architected Framework**\n- **Lab: Querying Data Using Athena**\n- **Knowledge Checks:**\n    - **Online Knowledge Check**\n    - **Sample Exam Question**\n\nThe module\'s objectives are explored in depth throughout these sections. \n\nYou will also engage in an activity applying the Well-Architected Framework and participate in a hands-on lab using Athena. \n\nFinally, the module concludes with a sample exam question and an online knowledge check to solidify your understanding of the presented material.\n\n### 3.2.4 AWS Well-Architected Framework and Lenses\n\nThis section introduces the Well-Architected Framework as a valuable tool for designing modern data architectures and analytics pipelines.\n\n### 3.2.5 Well-Architected Framework Pillars\n\nThe AWS Academy Cloud Foundations course provided an introduction to the Well-Architected Framework. This framework, available on the AWS website (link provided on your Content Resources page), offers a structured approach to designing and reviewing architectures.\n\nThe framework encompasses six pillars:\n\n- **Operational Excellence:** Focuses on running and monitoring systems effectively.\n- **Security:** Ensures the protection of data and systems.\n- **Reliability:** Guarantees that systems are available and perform as expected.\n- **Performance Efficiency:** Optimizes resource utilization for cost-effectiveness and performance.\n- **Cost Optimization:** Manages costs and maximizes value.\n- **Sustainability:** Promotes environmentally responsible practices.\n\nEach pillar provides best practices and design guidance to support informed decision-making during architecture design or review.\n\n### 3.2.6 Well-Architected Framework Lenses\n\n#### 3.2.6.1 Well-Architected Lenses\n\n- Extend the AWS Well-Architected Framework guidance to specific domains.\n- Offer insights derived from real-world case studies.\n\n#### 3.2.6.2 Data Analytics Lens\n\n- Provides key design elements for analytics workloads.\n- Includes reference architectures for common scenarios.\n\n#### 3.2.6.3 ML Lens\n\n- Addresses distinctions between application and machine learning (ML) workloads.\n- Offers a recommended ML lifecycle.\n\nThe Well-Architected Framework incorporates lenses, which extend its guidance to address specific domains. The Data Analytics Lens assists in understanding the key design elements of well-architected analytics workloads and provides recommendations for improvement. It also includes reference architectures for common scenarios. \n\nThe ML Lens highlights differences between non-ML and ML applications, presenting a recommended lifecycle for ML applications. This lens will be discussed further in the Processing Data for ML module.\n\n### 3.2.7 Activity: Using the Well-Architected Framework\n\n- This activity utilizes the Data Analytics Lens from the Well-Architected Framework to identify cloud best practices that data engineering teams should adhere to when building their data pipelines.\n- Detailed instructions for completing this activity are provided in your online course.\n\nYou can participate in this activity individually or with your class. Refer to the instructions provided by your instructor or within your online course module.\n\n### 3.2.8 Key Takeaways: AWS Well-Architected Framework and Lenses\n\n- The Well-Architected Framework provides best practices and design guidance across six pillars.\n- The Well-Architected Framework Lenses extend guidance to specific domains.\n- The Data Analytics Lens guides design decisions related to the characteristics of data (volume, velocity, variety, veracity, and value).\n\nHere are some key takeaways from this section:\n\n- The Well-Architected Framework serves as a valuable resource for best practices and design guidance, with lenses expanding its applicability to specific domains.\n- The Data Analytics Lens can be used to effectively guide the design of data pipelines to accommodate the specific characteristics of the data being processed.\n\n### 3.2.9 The Evolution of Data Architectures\n\nThis section explores the evolution of data stores from 1970 to the present.\n\n### 3.2.10 Application Architecture: Evolving Toward Distributed Systems\n\nApplication architectures have undergone significant changes in recent decades. Initially, companies relied on on-premises mainframes to handle critical applications. These mainframes combined compute and storage, providing uninterrupted operation for extended periods.\n\nLater, applications transitioned to a client-server architecture, where a server responded to requests from various clients. This allowed for more distributed systems. Clients and servers could be located on the same computer, but the separation facilitated scalability.  \n\nThe advent of the internet brought about the three-tier architecture.  Applications were split into functional groups:\n\n- **Presentation Tier:** Provided the user interface.\n- **Application Tier:** Handled business logic and processing.\n- **Data Tier:** Provided persistent, long-term storage.\n\nThis division further enhanced scalability by enabling independent scaling of each tier.\n\nThe most recent architectural trend, well-aligned with cloud best practices, is the move towards microservices. With microservices, applications are divided into distinct services based on functionality.  Instead of having a single application handling inventory and order history data, these functions would be separated into two distinct, domain-specific services. This approach allows independent scaling of services and fosters greater agility among development teams.\n\nEach of these architectural shifts aimed to enhance scalability and resilience through the division of applications.\n\n### 3.2.11 Data Stores: Adapting to Greater Data Variety\n\n#### 3.2.11.1 Hierarchical Databases\n\nHierarchical databases, due to their rigid structure, proved insufficient for managing complex data relationships.\n\n#### 3.2.11.2 Relational Databases\n\nRelational databases emerged as a significant advancement, offering improved capabilities for defining relationships among data. They have been a mainstay in many applications since their inception in 1970.\n\n#### 3.2.11.3 The Internet\'s Data Variety \n\nThe internet era brought a surge in data variety, much of which didn\'t fit well into the rigid structure of relational schemas.\n\n#### 3.2.11.4 Nonrelational Databases\n\nTo address this challenge, nonrelational databases were developed in the late 1990s. These databases provided more flexibility for handling less structured data.\n\n#### 3.2.11.5 Big Data and AI/ML \n\nThe growth of big data and AI/ML applications led to a desire for storing vast amounts of unstructured and semistructured data.\n\n#### 3.2.11.6 Data Lakes\n\nData lakes emerged as a solution to store raw data without the need for immediate structure.  This enabled efficient data collection and querying in its rawest form.\n\n#### 3.2.11.7 Cloud Microservices\n\nThe rise of cloud microservices fueled the demand for data stores tailored to specific data types and functions.\n\n#### 3.2.11.8 Purpose-Built Cloud Data Stores\n\nIn response, purpose-built cloud data stores emerged, allowing developers to optimize storage for specific components based on their data type and processing requirements. For example, ledger databases could be employed for financial transactions.\n\nIn parallel with the evolution of application architecture, how data is stored and accessed has also undergone significant changes.  This evolution provides context for understanding the tools and best practices employed in modern data architectures.\n\n### 3.2.12 Data Architectures: Handling Volume and Velocity\n\n#### 3.2.12.1 Relational Databases\n\nRelational databases, while effective for structured data, faced limitations in handling the increasing volume and velocity of data.\n\n#### 3.2.12.2 Data Warehouses and OLTP vs. OLAP Databases\n\nData warehouses emerged to address the need for separating operational reporting from transactional databases. They were optimized for read-heavy querying across large datasets. Online analytical processing (OLAP) databases are designed for reporting, while online transaction processing (OLTP) databases focus on transactions like order creation or ATM withdrawals.  The extract, transform, and load (ETL) process was introduced to extract, transform, and load data from OLTP databases into the data warehouse.\n\n#### 3.2.12.3 Nonrelational Databases\n\nNonrelational databases, with their flexibility in handling diverse data types, provided additional options for handling data volume and velocity.\n\n#### 3.2.12.4 Big Data Systems\n\nIn the 2000s, big data systems or frameworks emerged to overcome database bottlenecks caused by the sheer volume and velocity of incoming data. These frameworks distributed data across multiple nodes, handling failures automatically and enabling faster analysis.\n\n#### 3.2.12.5 Lambda Architecture and Streaming Solutions\n\nWhile big data systems addressed challenges in data volume and velocity, batch processing introduced a lag between data arrival and its inclusion in analytics results. The lambda architecture, proposed by Nathan Marz, combined batch processing with stream processing to support near real-time insights. This approach has become a standard way to process big data.\n\nAs data volume and velocity continued to increase, the need for real-time analysis and decision-making became more critical. Streaming solutions emerged to address the limitations of batch processing, enabling continuous processing of data as it arrives.\n\n### 3.2.13 Modern Data Architectures: Unifying Distributed Solutions\n\n#### 3.2.13.1 Relational Databases\n\nRelational databases continue to play a role in modern data architectures, particularly for structured data and operational reporting.\n\n#### 3.2.13.2 Data Warehouses and OLTP vs. OLAP Databases\n\nData warehouses remain essential for reporting and analysis, while OLTP databases support transactional applications.\n\n#### 3.2.13.3 Nonrelational Databases\n\nNonrelational databases are widely employed to handle unstructured and semistructured data, often used for AI/ML applications.\n\n#### 3.2.13.4 Big Data Systems\n\nBig data systems provide scalable and robust solutions for processing vast datasets.\n\n#### 3.2.13.5 Lambda Architecture and Streaming Solutions\n\nLambda architecture and streaming solutions enable near real-time insights and continuous processing.\n\n#### 3.2.13.6 Modern Data Architecture on AWS\n\nInstead of selecting a single "best" data store or architecture, modern approaches often incorporate a combination of elements.  The key is to adopt a three-pronged strategy:\n\n- **Modernize:** Update existing technology with newer, more efficient solutions.\n- **Unify:** Create a single source of truth by consolidating disparate data sources.\n- **Innovate:** Extract greater value from data through advanced analytics.\n\nThis approach aligns with the goals of a modern data architecture: storing data in a central location and making it readily accessible for analytics and AI/ML applications.\n\n### 3.2.14 Key Takeaways: The Evolution of Data Architectures\n\n- Data stores and architectures evolved to adapt to increasing demands in data volume, variety, and velocity.\n- Modern data architectures utilize different types of data stores to meet specific use cases.\n- The goal of modern architecture is to unify disparate sources to maintain a single source of truth.\n\nHere are key points to remember from this section:\n\n- Data stores and architectures have continually evolved to keep pace with the growing volume, variety, and velocity of data.\n- Modern data architectures utilize various data stores, each optimized for specific use cases.\n- The overarching objective is to unify disparate data sources to create a single, reliable source of truth.\n\n### 3.2.15 Modern Data Architecture on AWS\n\nThis section provides an overview of the components of a modern data architecture on AWS.\n\n### 3.2.16 Modern Data Architecture\n\n#### 3.2.16.1 Key Design Considerations\n\n- **Scalable Data Lake:** Enables storage and management of vast amounts of data.\n- **Performant and Cost-Effective Components:** Balances performance with cost-efficiency.\n- **Seamless Data Movement:** Facilitates data transfer between components.\n- **Unified Governance:** Ensures consistent data access control and management.\n\n#### 3.2.16.2 Big Data Processing: Handles large-scale data processing tasks.\n\n#### 3.2.16.3 Log Analytics: Analyzes log data for insights and troubleshooting.\n\n#### 3.2.16.4 Relational Databases: Stores and manages structured data.\n\n#### 3.2.16.5 Data Lake:  A centralized repository for raw data of various formats.\n\n#### 3.2.16.6 Data Warehousing: Optimizes data for reporting and analysis.\n\n#### 3.2.16.7 Nonrelational Databases: Stores and manages data without the constraints of relational schemas.\n\n#### 3.2.16.8 Machine Learning (ML): Enables the development and deployment of machine learning models.\n\nThe goal of a modern data architecture is to centralize data storage and make it accessible to all consumers for analytics and AI/ML applications.  However, this doesn\'t mean using a single data store; it\'s about having a unified source of truth.  As illustrated, a data lake serves as the central repository and integrates with other data stores and processing systems.  Data can be accessed directly from the lake or moved to and from other purpose-built tools for processing.\n\nThe diagram on this slide provides a conceptual view of the components of a modern data architecture and their interconnectedness. It is not a representation of the specific architecture for implementing these connections.\n\nData movement between the lake and other integrated services falls into three general types:\n\n- **Outside In:**  Moving data from purpose-built data stores (like data warehouses or databases) into the lake for analysis. For example, copying product sales data from a data warehouse into the lake to run product recommendation algorithms using ML.\n\n- **Inside Out:** Moving data from the lake to purpose-built data stores for additional ML or analytics.  For instance, collecting web application clickstream data directly into the lake and then moving a portion to a data warehouse for daily reporting.\n\n- **Around the Perimeter:** Moving data directly between data store components integrated with the lake, bypassing the lake itself.  For example, copying product catalog data from a database to a search service to facilitate product catalog exploration and reduce database search query load.\n\nThe modern data architecture approach addresses the strategies of modernizing, unifying, and innovating with your data architecture, but it also introduces significant complexity in building your data pipeline. A major challenge with a data lake architecture is the lack of oversight over the contents of raw data. For the lake to be truly useful, it needs defined mechanisms for cataloging and securing data. Without these mechanisms, data discovery and trust become problematic, potentially leading to a "data swamp."\n\nTo ensure the success of your modern data architecture, your design must incorporate the following features:\n\n- **Scalability:** The data lake should be able to scale easily as data grows. Use a scalable, durable data store that supports various data ingestion methods. For each component, select scalable services that strike a balance between high performance and minimal cost for the use case. \n- **Performance and Cost-Effectiveness:**  Choose purpose-built tools that optimize performance while minimizing costs. Regularly assess and explore options for enhancing performance or reducing costs.\n- **Seamless Data Movement:** Ensure smooth data transfer into and out of the lake and across the perimeter. Provide direct access to data whenever possible.\n- **Unified Governance:**  Maintain data consistency across all components of the architecture. As data volume increases, securely and governedly managing data movement becomes more challenging. This is referred to as data gravity.  Robust authorization and auditing mechanisms are crucial with a central location for policy definition and enforcement.\n\n### 3.2.17 AWS Purpose-Built Data Stores and Analytics Tools\n\n#### 3.2.17.1 Key Design Considerations\n\n- Scalable Data Lake\n- Performant and Cost-Effective Components\n\n#### 3.2.17.2 Amazon EMR: Provides a managed platform for big data processing.\n\n#### 3.2.17.3 Aurora: Offers a fully managed relational database engine optimized for the cloud.\n\n#### 3.2.17.4 Relational Databases: Manage structured data.\n\n#### 3.2.17.5 Big Data Processing: Handles large-scale data processing.\n\n#### 3.2.17.6 Nonrelational Databases: Manage data without the constraints of relational schemas.\n\n#### 3.2.17.7 DynamoDB: A fully managed, nonrelational database designed for high-performance applications.\n\n#### 3.2.17.8 Athena: Enables interactive querying of data directly in Amazon S3.\n\n#### 3.2.17.9 Log Analytics: Analyzes log data for insights and troubleshooting.\n\n#### 3.2.17.10 Amazon S3: Provides object storage for structured and unstructured data, serving as the foundation for data lakes on AWS.\n\n#### 3.2.17.11 Data Warehousing: Optimizes data for reporting and analysis.\n\n#### 3.2.17.12 Machine Learning: Enables the development and deployment of ML models.\n\n#### 3.2.17.13 SageMaker: Provides a managed platform for building, training, and deploying ML models.\n\n#### 3.2.17.14 Amazon Redshift: A fully managed data warehouse service.\n\nThis updated conceptual diagram highlights AWS services that align with the elements of the modern data architecture and support its key design considerations. You will delve deeper into each of these components throughout the course, and you will see how these connections are reflected in reference architectures.\n\nAmazon Simple Storage Service (Amazon S3) provides storage for structured and unstructured data, making it the ideal storage service for building data lakes on AWS. Amazon S3 offers cost-effective scaling, a secure environment, and high durability for data lakes of any size. It also enables the use of native AWS services for big data analytics and AI/ML applications.\n\nAmazon Athena is shown directly on the data lake to illustrate its ability to query data directly in Amazon S3.\n\nThe architecture demonstrates the following AWS purpose-built services that integrate with Amazon S3 and correspond to the components previously described:\n\n- Amazon Redshift: A fully managed data warehouse service.\n- Amazon OpenSearch Service: A purpose-built data store and search engine optimized for real-time analytics, including log analytics.\n- Amazon EMR: Simplifies big data processing by providing a managed platform.\n- Amazon Aurora: A relational database engine built for the cloud.\n- Amazon DynamoDB: A fully managed nonrelational database for high-performance applications.\n- Amazon SageMaker: An AI/ML service that democratizes access to ML processing.\n\nTo support seamless data movement, both Athena and Amazon Redshift support federated queries, allowing queries to be executed across different data stores without requiring data transfer between stores.\n\n### 3.2.18 AWS Services for Managing Data Movement and Governance\n\n#### 3.2.18.1 Key Design Considerations\n\n- Seamless Data Movement\n- Unified Governance\n\n#### 3.2.18.2 Amazon EMR: Provides a managed platform for big data processing.\n\n#### 3.2.18.3 Aurora: Offers a fully managed relational database engine optimized for the cloud.\n\n#### 3.2.18.4 Relational Databases: Manage structured data.\n\n#### 3.2.18.5 Big Data Processing: Handles large-scale data processing.\n\n#### 3.2.18.6 Nonrelational Databases: Manage data without the constraints of relational schemas.\n\n#### 3.2.18.7 Athena: Enables interactive querying of data directly in Amazon S3.\n\n#### 3.2.18.8 Log Analytics: Analyzes log data for insights and troubleshooting.\n\n#### 3.2.18.9 Amazon S3: Provides object storage for structured and unstructured data, serving as the foundation for data lakes on AWS.\n\n#### 3.2.18.10 Data Warehousing: Optimizes data for reporting and analysis.\n\n#### 3.2.18.11 DynamoDB: A fully managed, nonrelational database designed for high-performance applications.\n\n#### 3.2.18.12 Machine Learning: Enables the development and deployment of ML models.\n\n#### 3.2.18.13 SageMaker: Provides a managed platform for building, training, and deploying ML models.\n\n#### 3.2.18.14 Amazon Redshift: A fully managed data warehouse service.\n\n#### 3.2.18.15 Lake Formation: Simplifies the setup, access, and security of data lakes.\n\n#### 3.2.18.16 AWS Glue: Facilitates data movement and transformation between data stores, enabling faster preparation for analytics and ML.\n\nTwo additional services play a crucial role in managing data movement and governance within this architecture. \n\n- **AWS Glue:** Streamlines data movement and transformation between data stores, accelerating data preparation for analytics and ML compared to traditional ETL methods.\n\n- **AWS Lake Formation:** Simplifies time-consuming tasks associated with loading, monitoring, and managing data lakes. It assists in cataloging data, classifying it, and securing access for various users.\n\nThis conceptual view presents a wide range of infrastructure and pipeline components, but it\'s not intended to depict a detailed architectural blueprint.  However, it provides a useful reference point for understanding the goals of data storage and accessibility within an organization.\n\n### 3.2.19 Key Takeaways: Modern Data Architecture on AWS\n\n- A centralized data lake provides a single source of truth accessible to all consumers.\n- Purpose-built data stores and processing tools integrate seamlessly with the lake for data read and write operations.\n- The architecture supports three types of data movement: outside in, inside out, and around the perimeter.\n- Key AWS services facilitating seamless access to a centralized data lake include Amazon S3, Lake Formation, and AWS Glue.\n\nHere are some key takeaways from this section:\n\n- A data lake serves as a central repository, integrating with purpose-built data stores and processing tools.\n- Data can be moved into the lake (outside in), from the lake to other stores (inside out), or directly between purpose-built stores (around the perimeter).\n- Amazon S3 forms the foundation of the data lake, while Lake Formation and AWS Glue enhance seamless access.\n\n### 3.2.20 Modern Data Architecture Pipeline: Ingestion and Storage\n\nThe next two sections highlight aspects of the modern data architecture as reflected in the reference architecture presented in the Well-Architected Framework.  This section focuses on ingesting and storing data. For more detailed information, refer to the Modern Data Architecture section in Data Analytics Lens: AWS Well-Architected Framework, accessible through the link provided in your course resources. The slides here offer a high-level summary.\n\n### 3.2.21 Ingestion and Storage Layers in the Reference Architecture\n\n#### 3.2.21.1 Ingestion\n\n- Matches AWS services to the characteristics of the data source.\n- Integrates seamlessly with storage.\n\n#### 3.2.21.2 Storage\n\n- Provides durable and scalable storage.\n- Includes a metadata catalog for governance and data discoverability.\n\nThe layers of the reference architecture align with the pipeline layers introduced earlier in the course. The ingestion and storage layers make data accessible for processing and analysis.\n\nThe ingestion layer leverages individual, purpose-built AWS services to accommodate the unique connectivity, data format, data structure, and data velocity requirements of source types. This ensures data delivery to the storage layer components. These services include:\n\n- **Amazon AppFlow:**  Ingests data from SaaS applications, such as Salesforce or Zendesk.\n- **AWS Database Migration Service (AWS DMS):** Ingests data from operational databases like OLTP, ERP, CRM, and LOB databases.\n- **AWS DataSync:** Ingests data from file shares.\n- **Amazon Kinesis Data Streams and Amazon Kinesis Data Firehose:** Ingest data from streaming sources.\n\nYou will learn more about these services and their role in supporting ingestion of various data types in the Ingesting by Batch or by Stream module.\n\n### 3.2.22 Matching Ingestion Services to Variety, Volume, and Velocity\n\n#### 3.2.22.1 SaaS Apps: Software as a service applications.\n\n#### 3.2.22.2 OLTP: Online Transaction Processing databases.\n\n#### 3.2.22.3 ERP: Enterprise Resource Planning systems.\n\n#### 3.2.22.4 CRM: Customer Relationship Management systems.\n\n#### 3.2.22.5 LOB: Line of Business applications.\n\n#### 3.2.22.6 File Shares: Networked file storage systems.\n\n#### 3.2.22.7 Web: Web applications and websites.\n\n#### 3.2.22.8 Devices: Mobile devices, IoT sensors, etc.\n\n#### 3.2.22.9 Sensors: Data gathering devices like temperature sensors.\n\n#### 3.2.22.10 Social Media: Social media platforms.\n\nThe ingestion layer utilizes individual purpose-built AWS services to match the unique connectivity, data format, data structure, and data velocity requirements of various source types, ensuring efficient data delivery to the storage layer components.\n\n### 3.2.23 Modern Data Architecture Storage Layer\n\n#### 3.2.23.1 Storage Layer: Catalog\n\n- **AWS Glue Data Catalog:** Stores metadata about data sources.\n- **Lake Formation:** Simplifies data lake setup, access, and security.\n\n#### 3.2.23.2 Storage Layer: Storage\n\n- **Amazon Redshift:** A fully managed data warehouse service.\n- **Native Integration:** Enables seamless data movement between components.\n- **Amazon S3:** Object storage for structured and unstructured data.\n\nData is ingested into the storage layer.\n\nThe data storage layer provides durable, scalable, and cost-effective components for storing and managing massive amounts of data. In the AWS architecture, Amazon Redshift and Amazon S3 offer unified, natively integrated storage solutions.\n\nThe catalog layer within the storage layer is responsible for storing business and technical metadata about datasets hosted in the storage layer. This metadata facilitates finding and querying data stored in the data lake and data warehouse. In the AWS architecture, Lake Formation and AWS Glue collaborate to collect and store metadata, making it readily available. The catalog simplifies data discovery and exploration for consumers.\n\nThe following slides delve into the relationships between the data lake, data warehouse, and data catalog.\n\n### 3.2.24 Storage for Variety, Volume, and Velocity\n\n#### 3.2.24.1 Ingest: Data is brought into the storage layer.\n\n#### 3.2.24.2 Storage Layer: Storage\n\n- **Highly Structured Data:** Loaded into traditional schemas.\n- **Semistructured Data:**  Loaded into staging tables.\n- **Unstructured, Semistructured, and Structured Data:** Stored as objects.\n\n#### 3.2.24.3 Process:  Data is transformed for various use cases.\n\n- **Use Case:** Fast BI dashboards.\n    - **Service:** Amazon Redshift.\n- **Use Case:** Big data AI/ML.\n    - **Service:** Amazon S3.\n\nHighly structured data within the warehouse typically powers interactive queries and highly reliable, fast business intelligence (BI) dashboards. A modern cloud-native data warehouse, like Amazon Redshift, provides low-latency processing for complex SQL queries.\n\nData in a warehouse is usually ingested from highly structured sources like transactional systems and relational databases on a regular schedule. Amazon Redshift also supports ingesting semistructured data into staging tables.  For analytics, data in the warehouse needs to be highly reliable and structured according to traditional schemas.\n\nThe data lake primarily supports ML, data science, and big data processing use cases. It enables diverse datasets to be analyzed using various methods.\n\nThe Amazon S3 data lake supports storage in structured, semistructured, and unstructured formats and can scale automatically. Data can be ingested into the lake without defining a schema, speeding up data ingestion and making it available for exploration.  Both structured and unstructured data are stored as S3 objects.\n\nThe native integration between Amazon S3 and Amazon Redshift enables ingesting data directly into Amazon S3 and then preparing it for the data warehouse as needed. This allows for offloading historical data from warehouse storage into more cost-effective tiers in Amazon S3, reducing storage costs.\n\n### 3.2.25 Storage Zones for Data in Different States\n\n#### 3.2.25.1 Ingest: Data is brought into the storage layer.\n\n#### 3.2.25.2 Storage Layer: Storage\n\n- **Complex Querying:**  Supported by both Amazon S3 and Amazon Redshift.\n- **Data Zones in Amazon S3:**\n    - **Curated:** Highly processed and ready for consumption.\n    - **Trusted:**  Validated and conforming to established schemas.\n    - **Raw:**  Initial state, potentially unstructured or semistructured.\n    - **Landing:**  Initial data reception zone.\n\n#### 3.2.25.3 Process:  Data is enriched, validated, structured, and cleansed.\n\nThis slide provides a detailed look at how data can be ingested directly into Amazon S3 and then prepared for other use cases, including warehouse storage, using the concept of storage zones.\n\nEach zone represents a distinct data state and is represented by a bucket or prefix in Amazon S3. These zones include landing, raw, trusted, and curated. Data may progress through each zone as it undergoes cleansing, normalization, augmentation, or other transformations.\n\nTransformed data is saved into the zone that reflects its readiness for consumption.\n\nData entering the Amazon S3 data lake arrives at the landing zone. It is initially cleaned and stored in the raw zone for permanent storage.  Since data destined for the data warehouse needs to be highly reliable and conform to a schema, further processing is required.\n\nAdditional transformations include schema application, partitioning (structuring), and any other transformations necessary for the data to conform to the requirements of the trusted zone. Finally, the processing layer prepares the data for the curated zone by modeling and augmenting it for joining with other datasets (enrichment).  The transformed, validated data is stored in the curated layer. Datasets from the curated layer are ready to be ingested into the data warehouse, enabling low-latency access or complex SQL querying.\n\nThis process reflects the iterative nature of pipeline processing discussed earlier in the course. It also highlights how smaller pipelines operate within the larger architecture. The Ingesting and Preparing Data module delves deeper into these data transformation processes.\n\n### 3.2.26 Catalog Layer for Governance and Discoverability\n\n#### 3.2.26.1 Storage Layer: Catalog\n\n- **AWS Glue Data Catalog:** Stores metadata about data sources.\n- **Lake Formation:** Simplifies data lake setup, access, and security.\n\n#### 3.2.26.2 Storage Layer: Storage\n\n- **AWS Glue Crawlers:** Automatically discover schemas and metadata.\n- **Amazon S3:** Object storage for structured and unstructured data.\n- **Amazon Redshift:** A fully managed data warehouse service.\n    - **Schema Info:**  Provides schema information for data.\n- **Amazon Redshift Spectrum:** Enables querying data directly from Amazon S3.\n\nThis slide illustrates how the catalog layer supports governing access and discovering data in the lake or warehouse.\n\nThe AWS Glue service simplifies data movement and transformation within your pipeline. It can generate schemas for your data sources, which are then stored in an AWS Glue Data Catalog along with other metadata.  Another feature, AWS Glue data crawlers, automatically discover schemas and metadata about data in the data lake and data warehouse, updating the Data Catalog. You will use AWS Glue in the lab for this module and will learn more about it in the Ingesting by Batch or by Stream module.\n\nThe AWS Glue Data Catalog can also send schema and metadata information to Lake Formation, as depicted. Lake Formation is designed to streamline the setup, access, and security of data lakes.  It provides a central point for data lake administrators to establish granular permissions for databases and tables hosted within the data lake.\n\nIn this architecture, Lake Formation serves as the central catalog for all datasets hosted in Amazon S3 and Amazon Redshift. The catalog encompasses both business attributes (like data owner and column business definitions) and technical metadata (such as versioned table schemas and timestamps).\n\nAmazon Redshift Spectrum, a feature of Amazon Redshift, allows users to execute SQL queries that combine data from the data lake and the data warehouse. When a user issues a query request involving data from the data lake, Redshift Spectrum retrieves schema information from the Lake Formation catalog and uses it to query the data lake.\n\nThe integration between AWS Glue, Lake Formation, Amazon Redshift, and Amazon S3 simplifies the management and utilization of data ingested into your pipeline.\n\n### 3.2.27 Key Takeaways: Modern Data Architecture Pipeline: Ingestion and Storage\n\n- The AWS modern data architecture utilizes purpose-built tools to ingest data based on its characteristics.\n- The storage layer comprises two sublayers. The first is a storage layer utilizing Amazon Redshift for the data warehouse and Amazon S3 for the data lake. The second is a catalog layer that leverages AWS Glue and Lake Formation.\n- The catalog maintains metadata and provides schemas-on-read, enabling Redshift Spectrum to read data directly from Amazon S3.\n- Typically, data within the lake is divided into zones representing different processing stages. Data initially enters the landing zone and may progress to curated as it undergoes processing for various consumption purposes. Zones are created by using prefixes or bucket names for each zone in Amazon S3.\n\nHere are key points to remember from this section:\n\n- The AWS modern data architecture employs purpose-built tools to ingest data, considering its unique characteristics.\n- The storage layer has two sublayers. The first utilizes Amazon Redshift for the data warehouse and Amazon S3 for the data lake. The second is a catalog layer powered by AWS Glue and Lake Formation.\n- The catalog manages metadata and provides schemas-on-read, enabling Redshift Spectrum to directly access data from Amazon S3.\n- Data within the lake is typically segmented into zones representing different processing stages. Data initially enters the landing zone and may progress to curated as it undergoes processing for various consumption purposes. Zones are created by using prefixes or bucket names for each zone in Amazon S3.\n\n### 3.2.28 Modern Data Architecture Pipeline: Processing and Consumption\n\nThis section continues the discussion of the modern data architecture, focusing on the processing and consumption layers as documented in Data Analytics Lens: AWS Well-Architected Framework.\n\n### 3.2.29 Processing and Consumption Layers in the Reference Architecture\n\n#### 3.2.29.1 Processing\n\n- Transforms data into a consumable state.\n- Uses purpose-built components.\n\n#### 3.2.29.2 Analysis and Visualization (Consumption)\n\n- Democratizes consumption across the organization.\n- Provides unified access to stored data and metadata.\n\nThe processing and consumption layers in the modern data architecture prepare data and make it accessible to consumers. The consumption layer corresponds to the analysis and visualization layer of a data pipeline, as previously identified in the course.  In this reference architecture, it\'s called the consumption layer, reflecting that the available data can be used and consumed in diverse ways—by end users or by downstream systems that utilize the outputs of data processing.\n\n### 3.2.30 Modern Architecture Pipeline: Processing\n\n#### 3.2.30.1 Storage:  The data is stored in Amazon S3 or Amazon Redshift.\n\n#### 3.2.30.2 SQL-based ELT:  Data is transformed using SQL queries.\n\n#### 3.2.30.3 Big Data Processing:  Data is processed using big data tools.\n\n#### 3.2.30.4 Near Real-time ETL:  Data is processed near real-time.\n\n#### 3.2.30.5 Amazon Redshift: A fully managed data warehouse service.\n\n#### 3.2.30.6 Amazon EMR: A managed platform for big data processing.\n\n#### 3.2.30.7 AWS Glue: Facilitates data movement and transformation.\n\n#### 3.2.30.8 Kinesis Data Analytics:  Processes streaming data.\n\n#### 3.2.30.9 Spark Streaming on Amazon EMR or AWS Glue:  Processes streaming data using Spark.\n\n#### 3.2.30.10 Amazon S3: Object storage for structured and unstructured data.\n\n#### 3.2.30.11 Amazon Redshift: A fully managed data warehouse service.\n\n#### 3.2.30.12 Transform for Further Processing or Consumption: Data is processed for specific use cases.\n\nAs']